{"prompt": ["I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I want to traverse all subdirectories, except the node_modules directory.", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "I need to find the binary files in a directory. I want to do this with file, and after that I will check the results with grep. But my problem is that I have no idea what is a binary file. What will give the file command for binary files or what should I check with grep?", "Given a file containing this string:IT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*OK@IT1*1*CS*VN*ABC@SAC*X*500@REF*ZZ*BAR@IT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*BAR@IT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*OK@\nThe goal is to extract the following:IT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*BAR@\nWith the criteria being:\nThe IT1 \"line\" must contain *EA*\nThe REF line must contain BAR\nSome notes for consideration:\n\"@\" can be thought of as a line break\nA \"group\" of lines contains lines starting with IT1 and ending with REF\nI am running GNU grep 3.7.\nThe goal is to select the \"group\" of lines meeting the criteria.I tried the following:grep -oP \"IT1[^@]*EA[^@]*@.*REF[^@]*BAR[^@]*@\" file.txt\nBut it captures characters from the beginning of the example.Also tried to use lookarounds:grep -oP \"(?<=IT1[^@]*EA[^@]*@).*?(?=REF[^@]*BAR[^@]*@)\" file.txt\nBut my version of grep returns:\ngrep: lookbehind assertion is not fixed length\n", "What is the fastest way to extract a substring of interest from input such as the following?MsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nDesired output (i.e., the :-terminated string following the string MsgTrace(65/26) in this example):noopI tried the following, but without success:egrep -i \"[a-zA-Z]+\\(.*\\)[a-z]+:\"\n", "What is the fastest way to extract a substring of interest from input such as the following?MsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nDesired output (i.e., the :-terminated string following the string MsgTrace(65/26) in this example):noopI tried the following, but without success:egrep -i \"[a-zA-Z]+\\(.*\\)[a-z]+:\"\n", "Basically I'm wondering why this doesn't output anything:tail --follow=name file.txt | grep something | grep something_else \nYou can assume that it should produce output I have run another line to confirmcat file.txt | grep something | grep something_else\nIt seems like you can't pipe the output of tail more than once!? Anyone know what the deal is and is there a solution?EDIT:\nTo answer the questions so far, the file definitely has contents that should be displayed by the grep. As evidence if the grep is done like so:tail --follow=name file.txt | grep something\nOutput shows up correctly, but if this is used instead:tail --follow=name file.txt | grep something | grep something\nNo output is shown.If at all helpful I am running ubuntu 10.04", "Basically I'm wondering why this doesn't output anything:tail --follow=name file.txt | grep something | grep something_else \nYou can assume that it should produce output I have run another line to confirmcat file.txt | grep something | grep something_else\nIt seems like you can't pipe the output of tail more than once!? Anyone know what the deal is and is there a solution?EDIT:\nTo answer the questions so far, the file definitely has contents that should be displayed by the grep. As evidence if the grep is done like so:tail --follow=name file.txt | grep something\nOutput shows up correctly, but if this is used instead:tail --follow=name file.txt | grep something | grep something\nNo output is shown.If at all helpful I am running ubuntu 10.04", "Basically I'm wondering why this doesn't output anything:tail --follow=name file.txt | grep something | grep something_else \nYou can assume that it should produce output I have run another line to confirmcat file.txt | grep something | grep something_else\nIt seems like you can't pipe the output of tail more than once!? Anyone know what the deal is and is there a solution?EDIT:\nTo answer the questions so far, the file definitely has contents that should be displayed by the grep. As evidence if the grep is done like so:tail --follow=name file.txt | grep something\nOutput shows up correctly, but if this is used instead:tail --follow=name file.txt | grep something | grep something\nNo output is shown.If at all helpful I am running ubuntu 10.04", "Basically I'm wondering why this doesn't output anything:tail --follow=name file.txt | grep something | grep something_else \nYou can assume that it should produce output I have run another line to confirmcat file.txt | grep something | grep something_else\nIt seems like you can't pipe the output of tail more than once!? Anyone know what the deal is and is there a solution?EDIT:\nTo answer the questions so far, the file definitely has contents that should be displayed by the grep. As evidence if the grep is done like so:tail --follow=name file.txt | grep something\nOutput shows up correctly, but if this is used instead:tail --follow=name file.txt | grep something | grep something\nNo output is shown.If at all helpful I am running ubuntu 10.04", "Basically I'm wondering why this doesn't output anything:tail --follow=name file.txt | grep something | grep something_else \nYou can assume that it should produce output I have run another line to confirmcat file.txt | grep something | grep something_else\nIt seems like you can't pipe the output of tail more than once!? Anyone know what the deal is and is there a solution?EDIT:\nTo answer the questions so far, the file definitely has contents that should be displayed by the grep. As evidence if the grep is done like so:tail --follow=name file.txt | grep something\nOutput shows up correctly, but if this is used instead:tail --follow=name file.txt | grep something | grep something\nNo output is shown.If at all helpful I am running ubuntu 10.04", "Using an ls \u2013a and grep, how would you list the name of all of the files in /usr starting with the letter p or the letter r or the letter s using a single grep command? would this be right?ls \u2013a | grep [prs] /usr\n", "Using an ls \u2013a and grep, how would you list the name of all of the files in /usr starting with the letter p or the letter r or the letter s using a single grep command? would this be right?ls \u2013a | grep [prs] /usr\n", "Using an ls \u2013a and grep, how would you list the name of all of the files in /usr starting with the letter p or the letter r or the letter s using a single grep command? would this be right?ls \u2013a | grep [prs] /usr\n", "Using an ls \u2013a and grep, how would you list the name of all of the files in /usr starting with the letter p or the letter r or the letter s using a single grep command? would this be right?ls \u2013a | grep [prs] /usr\n", "Using an ls \u2013a and grep, how would you list the name of all of the files in /usr starting with the letter p or the letter r or the letter s using a single grep command? would this be right?ls \u2013a | grep [prs] /usr\n", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "Am trying to grep pattern from dozen files .tar.gz but its very slowam using tar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n", "I have a process that fails regularly & sometimes starts duplicate instances..When I run:\nps x |grep -v grep |grep -c \"processname\"\nI will get:\n2\nThis is normal as the process runs with a recovery process..If I get\n0\nI will want to start the process\nif I get:\n4\nI will want to stop & restart the processWhat I need is a way of taking the result of ps x |grep -v grep |grep -c \"processname\"Then setup a simple 3 option functionps x |grep -v grep |grep -c \"processname\"\nif answer = 0 (start process & write NOK & Time to log /var/processlog/check)\nif answer = 2 (Do nothing & write OK & time to log /var/processlog/check)\nif answer = 4 (stot & restart the process & write NOK & Time to log /var/processlog/check)\nThe process is stopped with\nkillall -9 process\nThe process is started with\nprocess -b -c /usr/local/etcMy main problem is finding a way to act on the result of ps x |grep -v grep |grep -c \"processname\".Ideally, I would like to make the result of that grep a variable within the script with something like this:process=$(ps x |grep -v grep |grep -c \"processname\")If possible.", "I have a process that fails regularly & sometimes starts duplicate instances..When I run:\nps x |grep -v grep |grep -c \"processname\"\nI will get:\n2\nThis is normal as the process runs with a recovery process..If I get\n0\nI will want to start the process\nif I get:\n4\nI will want to stop & restart the processWhat I need is a way of taking the result of ps x |grep -v grep |grep -c \"processname\"Then setup a simple 3 option functionps x |grep -v grep |grep -c \"processname\"\nif answer = 0 (start process & write NOK & Time to log /var/processlog/check)\nif answer = 2 (Do nothing & write OK & time to log /var/processlog/check)\nif answer = 4 (stot & restart the process & write NOK & Time to log /var/processlog/check)\nThe process is stopped with\nkillall -9 process\nThe process is started with\nprocess -b -c /usr/local/etcMy main problem is finding a way to act on the result of ps x |grep -v grep |grep -c \"processname\".Ideally, I would like to make the result of that grep a variable within the script with something like this:process=$(ps x |grep -v grep |grep -c \"processname\")If possible.", "I have a process that fails regularly & sometimes starts duplicate instances..When I run:\nps x |grep -v grep |grep -c \"processname\"\nI will get:\n2\nThis is normal as the process runs with a recovery process..If I get\n0\nI will want to start the process\nif I get:\n4\nI will want to stop & restart the processWhat I need is a way of taking the result of ps x |grep -v grep |grep -c \"processname\"Then setup a simple 3 option functionps x |grep -v grep |grep -c \"processname\"\nif answer = 0 (start process & write NOK & Time to log /var/processlog/check)\nif answer = 2 (Do nothing & write OK & time to log /var/processlog/check)\nif answer = 4 (stot & restart the process & write NOK & Time to log /var/processlog/check)\nThe process is stopped with\nkillall -9 process\nThe process is started with\nprocess -b -c /usr/local/etcMy main problem is finding a way to act on the result of ps x |grep -v grep |grep -c \"processname\".Ideally, I would like to make the result of that grep a variable within the script with something like this:process=$(ps x |grep -v grep |grep -c \"processname\")If possible.", "I have a process that fails regularly & sometimes starts duplicate instances..When I run:\nps x |grep -v grep |grep -c \"processname\"\nI will get:\n2\nThis is normal as the process runs with a recovery process..If I get\n0\nI will want to start the process\nif I get:\n4\nI will want to stop & restart the processWhat I need is a way of taking the result of ps x |grep -v grep |grep -c \"processname\"Then setup a simple 3 option functionps x |grep -v grep |grep -c \"processname\"\nif answer = 0 (start process & write NOK & Time to log /var/processlog/check)\nif answer = 2 (Do nothing & write OK & time to log /var/processlog/check)\nif answer = 4 (stot & restart the process & write NOK & Time to log /var/processlog/check)\nThe process is stopped with\nkillall -9 process\nThe process is started with\nprocess -b -c /usr/local/etcMy main problem is finding a way to act on the result of ps x |grep -v grep |grep -c \"processname\".Ideally, I would like to make the result of that grep a variable within the script with something like this:process=$(ps x |grep -v grep |grep -c \"processname\")If possible.", "I have a process that fails regularly & sometimes starts duplicate instances..When I run:\nps x |grep -v grep |grep -c \"processname\"\nI will get:\n2\nThis is normal as the process runs with a recovery process..If I get\n0\nI will want to start the process\nif I get:\n4\nI will want to stop & restart the processWhat I need is a way of taking the result of ps x |grep -v grep |grep -c \"processname\"Then setup a simple 3 option functionps x |grep -v grep |grep -c \"processname\"\nif answer = 0 (start process & write NOK & Time to log /var/processlog/check)\nif answer = 2 (Do nothing & write OK & time to log /var/processlog/check)\nif answer = 4 (stot & restart the process & write NOK & Time to log /var/processlog/check)\nThe process is stopped with\nkillall -9 process\nThe process is started with\nprocess -b -c /usr/local/etcMy main problem is finding a way to act on the result of ps x |grep -v grep |grep -c \"processname\".Ideally, I would like to make the result of that grep a variable within the script with something like this:process=$(ps x |grep -v grep |grep -c \"processname\")If possible.", "I have a process that fails regularly & sometimes starts duplicate instances..When I run:\nps x |grep -v grep |grep -c \"processname\"\nI will get:\n2\nThis is normal as the process runs with a recovery process..If I get\n0\nI will want to start the process\nif I get:\n4\nI will want to stop & restart the processWhat I need is a way of taking the result of ps x |grep -v grep |grep -c \"processname\"Then setup a simple 3 option functionps x |grep -v grep |grep -c \"processname\"\nif answer = 0 (start process & write NOK & Time to log /var/processlog/check)\nif answer = 2 (Do nothing & write OK & time to log /var/processlog/check)\nif answer = 4 (stot & restart the process & write NOK & Time to log /var/processlog/check)\nThe process is stopped with\nkillall -9 process\nThe process is started with\nprocess -b -c /usr/local/etcMy main problem is finding a way to act on the result of ps x |grep -v grep |grep -c \"processname\".Ideally, I would like to make the result of that grep a variable within the script with something like this:process=$(ps x |grep -v grep |grep -c \"processname\")If possible.", "I have a process that fails regularly & sometimes starts duplicate instances..When I run:\nps x |grep -v grep |grep -c \"processname\"\nI will get:\n2\nThis is normal as the process runs with a recovery process..If I get\n0\nI will want to start the process\nif I get:\n4\nI will want to stop & restart the processWhat I need is a way of taking the result of ps x |grep -v grep |grep -c \"processname\"Then setup a simple 3 option functionps x |grep -v grep |grep -c \"processname\"\nif answer = 0 (start process & write NOK & Time to log /var/processlog/check)\nif answer = 2 (Do nothing & write OK & time to log /var/processlog/check)\nif answer = 4 (stot & restart the process & write NOK & Time to log /var/processlog/check)\nThe process is stopped with\nkillall -9 process\nThe process is started with\nprocess -b -c /usr/local/etcMy main problem is finding a way to act on the result of ps x |grep -v grep |grep -c \"processname\".Ideally, I would like to make the result of that grep a variable within the script with something like this:process=$(ps x |grep -v grep |grep -c \"processname\")If possible.", "I have requirement to grep data with unique keyword and delete the upper line with below two lines.For example:Source file:{\n  name: main\n  phase: beta\n}\n{\n  name: test\n  phase: dev\n}\nI have used below command to grep the data:grep -A2 -B1 main sourcefile\nwhich gives me :{\n  name: main\n  phase: beta\n}\nI have used with -v option but no luck ... its still show whole data.grep -vA2 -vB1 main sourcefile\nThen I print the line numbers with grep and try to delete the same with sed but that also not working.for i in `grep -n -A2 -B1 main sourcefile | awk {'print $1'} | cut -b 1,2` do sed -e \"$i\"d sourcefile; done.\nAny help will be appreciated.trying to delete the data from a file with grep unique keyword and update the file", "I have requirement to grep data with unique keyword and delete the upper line with below two lines.For example:Source file:{\n  name: main\n  phase: beta\n}\n{\n  name: test\n  phase: dev\n}\nI have used below command to grep the data:grep -A2 -B1 main sourcefile\nwhich gives me :{\n  name: main\n  phase: beta\n}\nI have used with -v option but no luck ... its still show whole data.grep -vA2 -vB1 main sourcefile\nThen I print the line numbers with grep and try to delete the same with sed but that also not working.for i in `grep -n -A2 -B1 main sourcefile | awk {'print $1'} | cut -b 1,2` do sed -e \"$i\"d sourcefile; done.\nAny help will be appreciated.trying to delete the data from a file with grep unique keyword and update the file", "I have requirement to grep data with unique keyword and delete the upper line with below two lines.For example:Source file:{\n  name: main\n  phase: beta\n}\n{\n  name: test\n  phase: dev\n}\nI have used below command to grep the data:grep -A2 -B1 main sourcefile\nwhich gives me :{\n  name: main\n  phase: beta\n}\nI have used with -v option but no luck ... its still show whole data.grep -vA2 -vB1 main sourcefile\nThen I print the line numbers with grep and try to delete the same with sed but that also not working.for i in `grep -n -A2 -B1 main sourcefile | awk {'print $1'} | cut -b 1,2` do sed -e \"$i\"d sourcefile; done.\nAny help will be appreciated.trying to delete the data from a file with grep unique keyword and update the file", "I am new to grep and UNIX. I have a sample of data and want to display all the first names that only contain three characters e.g. Lee_example.  but I having some difficulty doing that. I am currently using this code  cat file.txt|grep -E \"[A-Z][a-z]{2}\" but it is displaying all the names that contain at least 3 characters and not only 3 charactersSample data\n\n\n\nname\nnumber\n\n\n\n\nLee_example\n1\n\n\nHector_exaple\n2\n\n\n\n", "I am new to grep and UNIX. I have a sample of data and want to display all the first names that only contain three characters e.g. Lee_example.  but I having some difficulty doing that. I am currently using this code  cat file.txt|grep -E \"[A-Z][a-z]{2}\" but it is displaying all the names that contain at least 3 characters and not only 3 charactersSample data\n\n\n\nname\nnumber\n\n\n\n\nLee_example\n1\n\n\nHector_exaple\n2\n\n\n\n", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns.", "I needed to find all the files that contained a specific string pattern. The first solution that comes to mind is using find piped with xargs grep:find . -iname '*.py' | xargs grep -e 'YOUR_PATTERN'\nBut if I need to find patterns that spans on more than one line, I'm stuck because vanilla grep can't find multiline patterns."], "chosen": ["\nSOLUTION 1 (combine find and grep)\nThe purpose of this solution is not to deal with grep performance but to show a portable solution : should also work with busybox or GNU version older than 2.5.\nUse find, for excluding directories foo and bar :\nfind /dir \\( -name foo -prune \\) -o \\( -name bar -prune \\) -o -name \"*.sh\" -print\n\nThen combine find and the non-recursive use of grep, as a portable solution :\nfind /dir \\( -name node_modules -prune \\) -o -name \"*.sh\" -exec grep --color -Hn \"your text to find\" {} 2>/dev/null \\;\n\nSOLUTION 2 (using the --exclude-dir option of grep):\nYou know this solution already, but I add it since it's the most recent and efficient solution. Note this is a less portable solution but more human-readable.\ngrep -R --exclude-dir=node_modules 'some pattern' /path/to/search\n\nTo exclude multiple directories, use --exclude-dir as:\n--exclude-dir={node_modules,dir1,dir2,dir3}\nSOLUTION 3 (Ag)\nIf you frequently search through code, Ag (The Silver Searcher) is a much faster alternative to grep, that's customized for searching code. For instance, it automatically ignores files and directories listed in .gitignore, so you don't have to keep passing the same cumbersome exclude options to grep or find.\n", "\nRecent versions of GNU Grep (>= 2.5.2) provide:\n--exclude-dir=dir\n\nwhich excludes directories matching the pattern dir from recursive directory searches.\nSo you can do:\ngrep -R --exclude-dir=node_modules 'some pattern' /path/to/search\n\nFor a bit more information regarding syntax and usage see\n\nThe GNU man page for File and Directory Selection\nA related StackOverflow answer Use grep --exclude/--include syntax to not grep through certain files\n\nFor older GNU Greps and POSIX Grep, use find as suggested in other answers.\nOr just use ack (Edit: or The Silver Searcher) and be done with it!\n", "\nIf you want to exclude multiple directories: \n\"r\" for recursive, \"l\" to print only names of files containing matches and \"i\" to ignore case distinctions :\ngrep -rli --exclude-dir={dir1,dir2,dir3} keyword /path/to/search\n\nExample : I want to find files that contain the word 'hello'. I want to search in all my linux directories except proc directory, boot directory, sys directory and root directory :\ngrep -rli --exclude-dir={proc,boot,root,sys} hello /\n\nNote : The example above needs to be root\nNote 2 (according to @skplunkerin) : do not add spaces after the commas in {dir1,dir2,dir3}\n", "\nThis syntax\n--exclude-dir={dir1,dir2}\n\nis expanded by the shell (e.g. Bash), not by grep, into this:\n--exclude-dir=dir1 --exclude-dir=dir2\n\nQuoting will prevent the shell from expanding it, so this won't work:\n--exclude-dir='{dir1,dir2}'    <-- this won't work\n\nThe patterns used with --exclude-dir are the same kind of patterns described in the man page for the --exclude option:\n--exclude=GLOB\n    Skip files whose base name matches GLOB (using wildcard matching).\n    A file-name glob can use *, ?, and [...]  as wildcards, and \\ to\n    quote a wildcard or backslash character literally.\n\nThe shell will generally try to expand such a pattern itself, so to avoid this, you should quote it:\n--exclude-dir='dir?'\n\nYou can use the curly braces and quoted exclude patterns together like this:\n--exclude-dir={'dir?','dir??'}\n\n", "\nIf you are grepping for code in a git repository and node_modules is in your .gitignore, you can use git grep. git grep searches the tracked files in the working tree, ignoring everything from .gitignore\ngit grep \"STUFF\"\n\n", "\nMany correct answers have been given here, but I'm adding this one to emphasize one point which caused some rushed attempts to fail before: exclude-dir takes a pattern, not a path to a directory.\nSay your search is:\ngrep -r myobject\n\nAnd you notice that your output is cluttered with results from the src/other/objects-folder. This command will not give you the intended result:\ngrep -r myobject --exclude-dir=src/other/objects-folder\n\nAnd you may wonder why exclude-dir isn't working! To actually exclude results from the objects-folder, simply do this:\ngrep -r myobject --exclude-dir=objects-folder\n\nIn other words, just use the folder name, not the path. Obvious once you know it.\nFrom the man page:\n\n--exclude-dir=GLOB\n         Skip any command-line directory with a name suffix  that  matches  the  pattern  GLOB.   When\n         searching  recursively,  skip  any  subdirectory  whose  base  name matches GLOB.  Ignore any\n         redundant trailing slashes in GLOB.\n\n", "\nFrequently use this:\ngrep can be used in conjunction with -r (recursive), i (ignore case) and -o (prints only matching part of lines). To exclude files use --exclude and to exclude directories use --exclude-dir.\nPutting it together you end up with something like:\ngrep -rio --exclude={filenames comma separated} \\\n--exclude-dir={directory names comma separated} <search term> <location>\n\nDescribing it makes it sound far more complicated than it actually is. Easier to illustrate with a simple example.\nExample: \nSuppose I am searching for current project for all places where I explicitly set the string value debugger during a debugging session, and now wish to review / remove. \nI write a script called findDebugger.sh and use grep to find all occurrences. However:\nFor file exclusions - I wish to ensure that .eslintrc is ignored (this actually has a linting rule about debugger so should be excluded). Likewise, I don't want my own script to be referenced in any results. \nFor directory exclusions - I wish to exclude node_modules as it contains lots of libraries that do reference debugger and I am not interested in those results. Also I just wish to omit .idea and .git hidden directories because I don't care about those search locations either, and wish to keep the search performant.\nSo here is the result - I create a script called findDebugger.sh with:\n#!/usr/bin/env bash\ngrep -rio --exclude={.eslintrc,findDebugger.sh} \\\n--exclude-dir={node_modules,.idea,.git} debugger .\n\n", "\nYou could try something like grep -R search . | grep -v '^node_modules/.*'\n", "\nVery useful, especially for those dealing with Node.js where we want to avoid searching inside \"node_modules\":\nfind ./ -not -path \"*/node_modules/*\" -name \"*.js\" | xargs grep keyword\n\n", "\nStep 1:\nvim ~/.bash_profile\nsearch() {\n    grep -InH -r --exclude-dir=*build* -e \"$1\" .\n}\n\nStep 2:\nsource ~/.bash_profile\nUsage:\nsearch \"<string_to_be_searched>\"\n", "\nA simple working command:\nroot/dspace# grep -r --exclude-dir={log,assetstore} \"creativecommons.org\"\n\nAbove I grep for text \"creativecommons.org\" in current directory \"dspace\" and exclude dirs {log,assetstore}.\nDone.\n", "\nfind . ! -name \"node_modules\" -type d \n\n", "\nThis one works for me:\ngrep <stuff> -R --exclude-dir=<your_dir>\n\n", "\nHere is my solution:\n4.1\nfor i in {1..10}; do openssl rand -base64 6 | tr -dc 'a-zA-Z0-9' | head -c 8 >> best_secrets.txt && echo >> best_secrets.txt; done\n4.2\n#!/bin/bash\nif [ $# -ne 1 ]; then\necho \"Usage: $0 \"\nexit 1\nfi\ndd if=/dev/urandom of=\"$1\" bs=1M count=150\n4.3\nalias change_me='chsh -s $(which zsh)'\n4.4\n#!/bin/bash\nif [ $# -ne 1 ]; then\necho \"Usage: $0 \"\nexit 1\nfi\ngrep \"^$1:\" /etc/shadow | cut -d':' -f2\n1\nHere is my solution:\n4.1\nawk -F: '$3 > 1000 {print $1}' /etc/passwd\n\n4.2\nsudo useradd uso\necho \"uso:fancy-password\" | sudo chpasswd\n\n4.3\nsort -t: -k7,7 -k3,3n /etc/passwd\n\n4.4\nfree -h | grep Mem | awk '{print $4}'\n\n2\n", "\nThis finds all non-text based, binary, and empty files.\nEdit\nSolution with only grep (from Mehrdad's comment):\ngrep -rIL .\n\n\nOriginal answer\nThis does not require any other tool except find and grep:\nfind . -type f -exec grep -IL . \"{}\" \\;\n\n-I tells grep to assume binary files as unmatched\n-L prints only unmatched files\n. matches anything else\n\nEdit 2\nThis finds all non-empty binary files:\nfind . -type f ! -size 0 -exec grep -IL . \"{}\" \\;\n\n", "\nJust have to mention Perl's -T test for text files, and its opposite -B for binary files.\n$ find . -type f | perl -lne 'print if -B'\n\nwill print out any binary files it sees. Use -T if you want the opposite: text files.\nIt's not totally foolproof as it only looks in the first 1,000 characters or so, but it's better than some of the ad-hoc methods suggested here. See man perlfunc for the whole rundown. Here is a summary:\n\nThe \"-T\" and \"-B\" switches work as follows.  The first block or so of\nthe file is examined to see if it is valid UTF-8 that includes\nnon-ASCII characters.  If, so it's a \"-T\" file.  Otherwise, that same\nportion of the file is examined for odd characters such as strange\ncontrol codes or characters with the high bit set.  If more than a\nthird of the characters are strange, it's a \"-B\" file; otherwise it's\na \"-T\" file.  Also, any file containing a zero byte in the examined\nportion is considered a binary file.\n\n", "\nIn these modern times (2020 is practically the 3rd decade of the 21st century after all),\nI think the correct question is how do I find all the non-utf-8 files? Utf-8 being the modern equivalent of a text file.\nutf-8 encoding of text with non-ascii code points will introduce non-ascii bytes (i.e., bytes with the most significant bit set). Now, not all sequences of such bytes form valid utf-8 sequences.\nisutf8 from the moreutils package is what you need.\n$ isutf8 -l /bin/*\n/bin/[\n/bin/acyclic\n/bin/addr2line\n/bin/animate\n/bin/applydeltarpm\n/bin/apropos\n\u22ee\n\nA quick check:\n$ file $(isutf8 -l /bin/*)\n/bin/[:             ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=4d70c2142fc672d8a69d033ecb6693ec15b1e6fb, for GNU/Linux 3.2.0, stripped\n/bin/acyclic:       ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=d428ea52eb0e8aaf7faf30914710d8fbabe6ca28, for GNU/Linux 3.2.0, stripped\n/bin/addr2line:     ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=797f42bc4f8fb754a49b816b82d6b40804626567, for GNU/Linux 3.2.0, stripped\n/bin/animate:       ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=36ab46e69c1bfea433382ffc9bbd9708365dac2b, for GNU/Linux 3.2.0, stripped\n/bin/applydeltarpm: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=a1fddcbeec9266e698782596f2dfd1b4f3e0b974, for GNU/Linux 3.2.0, stripped\n/bin/apropos:       symbolic link to whatis\n\u22ee\n\nYou may wish to invert the test and get all the text files.\nUse -i:\n$ isutf8 -il /bin/*\n/bin/alias\n/bin/bashbug\n/bin/bashbug-64\n/bin/bg\n\u22ee\n$ file -L $(isutf8 -il /bin/*)\n/bin/alias:      a /usr/bin/sh script, ASCII text executable\n/bin/bashbug:    a /usr/bin/sh - script, ASCII text executable, with very long lines\n/bin/bashbug-64: a /usr/bin/sh - script, ASCII text executable, with very long lines\n/bin/bg:         a /usr/bin/sh script, ASCII text executable\n\u22ee\n\nYeah, it reads the whole file, but it's pretty speedy, and if you want accuracy\u2026\n", "\nAs this is an assignment, you would probably hate me if I gave you the complete solution ;-) So here is a little hint:\nThe grep command will output a list of binary files per default, if you search for a regular expression like . that will match on any non-empty file:\ngrep . *\n\nOutput:\n[...]\nBinary file c matches\nBinary file e matches\n\nYou can use awk to get the filenames only and ls to print the permissions. See the respective man pages (man grep, man awk, man ls).\n", "\nMy first answer to the question fell pretty much inline here using the find command.  I think your instructor was looking to get you into the concept of magic numbers using the file command, which breaks them down into multiple types. \nFor my purposes, it was as simple as:\nfile * | grep executable\n\nBut it can be done in numerous ways.\n", "\nThe answer that uses a combination of find and grep works, but it is very slow because it creates a new process for every file. The following solution is more efficient:\ncomm -2 -3 <(find . -type f -not -empty | sort) <(grep -rIl . . | sort)\n\n", "\nA late answer from the far future. The first issue here is that the problem is not well-defined. The term \"binary file\" is unclear, and the OP seems to be confused by that.\nWhat are \"binary files\"?\nI will go ahead and agree with Wikipedia here:\n\nA binary file is a computer file that is not a text file.[1] The term \"binary file\" is often used as a term meaning \"non-text file\".\n\nWhat is a text file, if not a binary file? To identify a text file one would need to know the encoding in advance, otherwise the file simple looks like an unknown binary file.\nMy go-to tool for answering the question \"what kind of file is this?\" is the file utility. This utility is smart enough to try to read the files with different encodings to see if it makes sense:\n\nIf a file does not match any of the entries in the magic file, it\nis examined to see if it seems to be a text file.  ASCII,\nISO-8859-x, non-ISO 8-bit extended-ASCII character sets (such as\nthose used on Macintosh and IBM PC systems), UTF-8-encoded Unicode,\nUTF-16-encoded Unicode, and EBCDIC character sets can be\ndistinguished by the different ranges and sequences of bytes that\nconstitute printable text in each set.\n\nIf a file is not of \"text\" type, then it must be \"binary\", according to the Wikipedia definition.\nThere are, however, 2 types of binary files that the file utility can detect:\n\nExecutable: mostly ELF files in Linux systemd, but there are other well known binary types\nData: everything else\n\nListing \"binary\" files in a directory, recursively\nThe following takes care of many edge cases, like file names that:\n\nHave the word \"ELF\" or \"data\" in the name\nHave white spaces in the name (though a newline character would make it break)\nHave a : in the name\n\nshopt -s globstar\nfile -0 **/* | sed -nE 's/\\x0:\\s*(ELF|data).*//p'\n\nCaveats\nI assumed that we are mostly searching for ELF files as our executable and library format. There are other competing formats, such as COFF and PE, so those would not be detected.\n", "\nI think the best tool to determine the nature of a file is the file utility.\nIn one of my directories I have only one file identified as binary by the nautilus file manager.\nFor this file only, the command ls | xargs file returns \"data\" without any further information.\n", "\nBinary files in linux have the format of ELF\nWhen you run file command on a binary file, then the output contains the word ELF. You can grep this.\nOn command line:\nfile <binary_file_name>\nSo, if you want to find the binary files inside a directory (in linux for example), you can do something like this:\nls | xargs file | grep ELF\n", "\nYou don't need PCREs for this, a simple POSIX ERE will do:\n$ grep -oE 'IT1[^@]*EA*[^@]*@[^@]*@REF[^@]*BA[^@]*@' file\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*BAR@\n\nIf I had to operate on that data, though, I wouldn't use grep on it like that as the regexp becomes lengthy and grep would read the whole input file into memory at once so YMMV with large input files.\nInstead I'd use awk to treat it as 3-line records of @-separated fields and then you can trivially do whatever you like with the fields and/or whole records, e.g. using GNU awk for multi-char RS and RT:\n$ awk -v RS='([^@]*@){3}' -F'@' '{$0=RT} ($1 ~ /EA/) && ($3 ~ /BAR/)' file\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*BAR@\n\nThe above only reads 3 @-separated strings at a time into memory and breaks down the input into these records and fields:\n$ awk -v RS='([^@]*@){3}' -F'@' 'RT{$0=RT; print; for (i=1; i<=NF; i++) print \"\\t\" i, $i}' file\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*OK@\n        1 IT1*1*EA*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*OK\n        4\nIT1*1*CS*VN*ABC@SAC*X*500@REF*ZZ*BAR@\n        1 IT1*1*CS*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*BAR\n        4\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*BAR@\n        1 IT1*1*EA*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*BAR\n        4\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*OK@\n        1 IT1*1*EA*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*OK\n        4\n\nThere's an empty field at the end of each record due to the @ at the end of each record. That can be trivially handled however you like (removed, ignored, kept, whatever).\nIf you don't have GNU awk you can do the same using any awk with just slightly more code:\n$ awk -v RS='@' -F'@' '{$0=prev $0 RS; prev=(NR%3 ? $0 : \"\")} !prev && ($1 ~ /EA/) && ($3 ~ /BAR/)' file\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*BAR@\n\n\n$ awk -v RS='@' -F'@' '{$0=prev $0 RS; prev=(NR%3 ? $0 : \"\")} !prev{print; for (i=1; i<=NF; i++) print \"\\t\" i, $i}' file\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*OK@\n        1 IT1*1*EA*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*OK\n        4\nIT1*1*CS*VN*ABC@SAC*X*500@REF*ZZ*BAR@\n        1 IT1*1*CS*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*BAR\n        4\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*BAR@\n        1 IT1*1*EA*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*BAR\n        4\nIT1*1*EA*VN*ABC@SAC*X*500@REF*ZZ*OK@\n        1 IT1*1*EA*VN*ABC\n        2 SAC*X*500\n        3 REF*ZZ*OK\n        4\n\n", "\nLittle quick and dirty test on a 2469120 lines text of such a sample entry give grep -PO as winner\ntime sed -n -e 's/^MsgTrace[^)]\\{4,\\})//;t M' -e 'b' -e ':M' -e 's/:.*//p' YourFile >/dev/null\nreal    0m7.61s\nuser    0m7:10s\nsys     0m0.13s\n\ntime awk -F ':' '/^MsgTrace/{ sub( /.*)/, \"\", $1); print $1}' YourFile >/dev/null\nreal    0m17.43s\nuser    0m16.19s\nsys     0m0.17s\n\ntime grep -Po  \"[a-zA-Z]\\(.*\\)\\K[a-z]+(?=:)\" YourFile >/dev/null\nreal    0m6.72s\nuser    0m6.23s\nsys     0m0.11s\n\ntime sed -n 's/[[:alpha:]]*([^)]*)\\([[:lower:]]*\\):.*/\\1/p' YourFile >/dev/null\nreal    0m17.43s\nuser    0m16.29s\nsys     0m0.12s\n\ntime grep -Po '(?<=MsgTrace\\(65/26\\)).*?(?=:)' YourFile >/dev/null\nreal    0m16.38s\nuser    0m15.22s\nsys     0m0.15s\n\nfor @EdMorton question (i redo the same original sed to have compare value in same context of machine load). The exact string is lot faster, i imagine that sed try several combination before selecting which is the longest one for all criteria where a .*l give lot more possibility than pool is full\ntime sed -n -e 's/^MsgTrace([^)]\\{3,\\})//;T' -e 's/:.*//p' YourFile >/dev/null\nreal    0m7.28s\nuser    0m6.60s\nsys     0m0.13s\n\ntime sed -n -e 's/^[[:alpha:]]*([^)]\\{3,\\})//;T' -e 's/:.*//p' YourFile >/dev/null\nreal    0m10.44s\nuser    0m9.67s\nsys     0m0.14s\n\ntime sed -n -e 's/^[[:alpha:]]*([^)]*)//;T' -e 's/:.*//p' YourFile >/dev/null\n\nreal    0m10.54s\nuser    0m9.75s\nsys     0m0.11s\n\n", "\nYou could try this:\n$ sed -n 's/[[:alpha:]]*([^)]*)\\([[:lower:]]*\\):.*/\\1/p' file\nnoop\n\nIt's portable to all POSIX seds and doesn't employ PCREs, just BREs, so the regexp matching part at least should be fast.\nAlternatively, using GNU awk for the 3rd arg to match():\n$ awk 'match($0,/[[:alpha:]]*\\([^)]*)([[:lower:]]*):/,a){print a[1]}' file\nnoop\n\n", "\nFigured out what was going on here. It turns out that the command is working it's just that the output takes a long time to reach the console (approx 120 seconds in my case). This is because the buffer on the standard out is not written each line but rather each block. So instead of getting every line from the file as it was being written I would get a giant block every 2 minutes or so.\nIt should be noted that this works correctly:\ntail file.txt | grep something | grep something\n\nIt is the following of the file with --follow=name that is problematic. \nFor my purposes I found a way around it, what I was intending to do was capture the output of the first grep to a file, so the command would be:\ntail --follow=name file.txt | grep something > output.txt\n\nA way around this is to use the script command like so:\nscript -c 'tail --follow=name file.txt | grep something' output.txt\n\nScript captures the output of the command and writes it to file, thus avoiding the second pipe.\nThis has effectively worked around the issue for me, and I have explained why the command wasn't working as I expected, problem solved.\nFYI, These other stackoverflow questions are related:\nTrick an application into thinking its stdin is interactive, not a pipe\nForce another program's standard output to be unbuffered using Python\n", "\nYou might also run into a problem with grep buffering when inside a pipe.\nie, you don't see the output from \n   tail --follow=name file.txt | grep something > output.txt\n\nsince grep will buffer its own output. \nUse the --line-buffered switch for grep to work around this:\ntail --follow=name file.txt | grep --line-buffered something > output.txt\n\nThis is useful if you want to get the results of the follow into the output.txt file as rapidly as possible. \n", "\nThis is also a problem with fish shell, where invoking \"grep\" invokes /usr/share/fish/functions/grep.fish, and the second grep is not being run at all, for example:\n>> sudo tail -f /var/log/syslog | grep kernel | grep --nonexistentoption\n\nproduces no output.\nThe fix then, is to sudo rm /usr/share/fish/functions/grep.fish\n", "\nYou do know that tail starts by default with the last ten lines of the file?  My guess is everything the cat version found is well into the past.  Try tail -n+1 --follow=name file.txt to start from the beginning of the file.\n", "\nworks for me on Mac without --follow=name\nbash-3.2$ tail delme.txt | grep po\nposition.bin\nposition.lrn\nbash-3.2$ tail delme.txt | grep po | grep lr\nposition.lrn\n\n", "\n ls -a /usr | grep '^[prs]'\n\nWould select from the output of ls -a /usr (which is the list of files in /usr delimited by newline characters) the lines that start by either of the p, r or s characters.\nThat's probably what your teacher is expecting but it's wrong or at least not reliable.\nFile names can be made of many lines since the newline character is as valid a character as any in a file name on Linux or any unix. So that command doesn't return the files whose name starts with p, q or s, but the lines of the filenames that start with p, q or s. Generally, you can't post-process the output of ls reliably.\n-a is to include hidden files, that is files whose name starts with .. Since you only want those that start with p, q or s, that's redundant.\nNote that:\nls /usr | grep ^[pqs]\n\nwould be even more wrong. First ^ is a special character in a few shells like the Bourne shell, rc, es or zsh -o extendedglob (though OK in bash or other POSIX shells).\nThen, in most shells (fish being a notable exception), [pqs] is a globbing operator. That means that ^[qps] is meant to be expanded by the shell to the list of files that match that pattern (relative to the current directory).\nSo in those shells like bash that don't treat ^ specially, if there is a file called ^p in the current directory, that will become\nls /usr | grep ^p\n\nIf there's no matching file, in csh, tcsh, zsh or bash -O failglob, you'll get an error message and the command will be cancelled. In zsh -o extendedglob where ^ is a globbing operator, ^[pqs] would mean any file but p, q or s.\n", "\nIf you're trying to find files, don't use ls.  Use the find command.\nfind /usr -name '[prs]*'\n\nIf you don't want to search the entire tree under /usr, do this:\nfind /usr -maxdepth 1 -name '[prs]*'\n\n", "\nThe comment of @twalberg is perfect for the options you asked.\nThe echo ignores hidden files, but those will be filtered when you only want files starting with [prs].\nWhen you want hidden files for other filters, a similar solution would be ls -ad /usr/[prs]*, where the -d option suppresses the listing of subdirs.\nThat solution will also show the full path, this can be suppressed when you go to the dir first. When you want to stay in your current directory, use a subshell for it.  I use && for skipping the ls when the /usr dir doesn't exist.\n(cd /usr && ls -ad [prs]*)\n\n", "\nI'd ls the directory and only then apply the grep. Also, you're missing the ^ character to filter out only the files that start with those letters:\n$ ls -a /usr | grep ^[prs]\n\n", "\nWhy not just\n find /usr -type f -name '[prs]*'\n\n", "\nThis is how you'd do it in awk. I'd be interested to see if the duration is longer or shorter.\nawk -F, '\n  NR==FNR { words[$1]; next }\n  $1 in words\n' keywords.csv big_csv.csv > output.csv\n\nIt works by reading the first file into an array using the common \"first file\" test, then checking that array as a key to the second file.\nI can update this further if you add more detail to your question.\n", "\nAssumptions:\n\nfirst field (both files) does not contain commas and is not wrapped in double quotes\ndata from keywords.csv may have windows/dos line endings (\\r)\n\nSetup:\n$ cat keywords.csv                         # run through unix2dos to add \"\\r\"\nADLV\nADVG\n\n$ cat big_csv.csv\nADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nWXYZ,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\n\nA basic awk approach:\nawk -F, '                                  # input field delimiter is a comma; for 1st file this implies entire line == 1st field\nFNR==NR { sub(/\\r$/,\"\"); a[$0]; next }     # 1st file: strip off \"\\r\", save line as index in array a[]; skip to next line of input (from 1st file)\n$1 in a                                    # 2nd file: if 1st field is an index in array a[] then print current line to stdout\n' keywords.csv big_csv.csv > output.csv\n\nThis generates:\n$ cat output.csv\nADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\n\n", "\nAssumption: key is in column 1 in both files (big.csv, keys.txt from your sample data)\nawk 'NR==FNR { a[$1] = 1; next } a[$1] {print $1,$2}' keys.txt FS=\\, big.csv\n\ngives:\nSDGA -1.678247\nSDSV -2.140182\nWDGV -1.31453\n\n(for the sake of readability I just printed the first two columns for matching keys)\nI'm not a native speaker (English), so for an explanation of NR, FNR, Arrays and all the stuff going on here, I would like to refer to this discussion.\nHope this helps!\n", "\nThis can also be done in Ruby:\nruby -e '\n# split(/\\R/) works the same with DOS or Unix line endings\nkeys=File.open(ARGV[0]).read.split(/\\R/).map(&:downcase).to_set\nFile.open(ARGV[1]).each_line{|line| \n    tst=line.split(/,/,2)[0].downcase\n    puts line if keys.include?(tst)\n}\n' sample_keyword.csv sample_input.csv >out.csv\n\nThere is no particular advantage to doing the basic example in Ruby. In fact I would do it in awk if the actual use is only what you have described. Ruby is potentially faster but often times not.\nHowever, you can do things like output to a different format (JSON, XML, complex csv) that are challenging in awk more easily done in Ruby.\nYou can replicate curl internally in Ruby and read your gist examples directly:\nruby -e '\nrequire \"net/http\"\nrequire \"uri\"\n\nuri1 = URI.parse(\"https://gist.githubusercontent.com/fishnibble/9d95658c352a1acab3cec3e965defb3f/raw/21fc5153a0b78cdb3eab88c72d700cdf74f20ae7/sample_keyword.csv\")\nkeys = Net::HTTP.get(uri1).split(/\\R/).map(&:downcase).to_set\n\n# This can be done in a streaming mode for huge data...\nuri2 = URI.parse(\"https://gist.githubusercontent.com/fishnibble/9d95658c352a1acab3cec3e965defb3f/raw/21fc5153a0b78cdb3eab88c72d700cdf74f20ae7/sample_input.csv\")\nNet::HTTP.get(uri2).split(/\\R/).each{|line|\n    tst=line.split(/,/,2)[0].downcase\n    puts line if keys.include?(tst)\n}' >out.csv\n\nThat is where you want Ruby / Python / Perl since that stream is hard to do in awk.  You can also mount external servers or read ftp, etc where that is challenging with awk alone.\n", "\n\nAre there anyway I could optimize this? Anything I'm missing?\n\nYou commanded GNU grep to look for keyword in whole line. This is unnecessary as you want to find lines having said keyword in 1st column, for CSV without quotes in 1st column this would mean line starting with keyword followed by comma (,) character.\n\nWould it be better to use awk or another tool for this?\n\nIf you have limited time you should prepare smaller example and then measure various solutions. As awk solution were already shown I will propose GNU sed solution by reworking your keywords file, say keywords.txt\nADLV\nADVG\n\ninto sed file by doing sed -e 's/^/\\/^/' -e 's/[\\r\\n]*$/,\\/p/' keywords.txt > file.sed\n/^ADLV,/p\n/^ADVG,/p\n\nwhich will create file.sed which could be used as follows\nsed -n -f file.sed file.csv\n\nExplanation: -n disengage default print action -f file.sed execute commands in file.sed, in this case these are just prints.\n(tested in GNU sed 4.8)\n", "\nYou can use the --to-command option to pipe files to an arbitrary script. Using this you can process the archive in a single pass (and without a temporary file). See also this question, and the manual.\nArmed with the above information, you could try something like:\n$ tar xf file.tar.gz --to-command \"awk '/bar/ { print ENVIRON[\\\"TAR_FILENAME\\\"]; exit }'\"\nbfe2/.bferc\nbfe2/CHANGELOG\nbfe2/README.bferc\n\n", "\nIf you have zgrep you can use\nzgrep -a string file.tar.gz\n\n", "\nI know this question is 4 years old, but I have a couple different options:\nOption 1: Using tar --to-command grep\nThe following line will look in example.tgz for PATTERN. This is similar to @Jester's example, but I couldn't get his pattern matching to work. \ntar xzf example.tgz --to-command 'grep --label=\"$TAR_FILENAME\" -H PATTERN ; true'\n\nOption 2: Using tar -tzf\nThe second option is using tar -tzf to list the files, then go through them with grep. You can create a function to use it over and over:\ntargrep () {\n    for i in $(tar -tzf \"$1\"); do\n        results=$(tar -Oxzf \"$1\" \"$i\" | grep --label=\"$i\" -H \"$2\")\n        echo \"$results\"\n    done\n}\n\nUsage:\ntargrep example.tar.gz \"pattern\"\n\n", "\nBoth the below options work well.\n$ zgrep -ai 'CDF_FEED' FeedService.log.1.05-31-2019-150003.tar.gz | more\n2019-05-30 19:20:14.568 ERROR 281 --- [http-nio-8007-exec-360] DrupalFeedService  : CDF_FEED_SERVICE::CLASSIFICATION_ERROR:408: Classification failed even after maximum retries for url : abcd.html\n\n$ zcat FeedService.log.1.05-31-2019-150003.tar.gz | grep -ai 'CDF_FEED'\n2019-05-30 19:20:14.568 ERROR 281 --- [http-nio-8007-exec-360] DrupalFeedService  : CDF_FEED_SERVICE::CLASSIFICATION_ERROR:408: Classification failed even after maximum retries for url : abcd.html\n\n", "\nIf this is really slow, I suspect you're dealing with a large archive file.  It's going to uncompress it once to extract the file list, and then uncompress it N times--where N is the number of files in the archive--for the grep.  In addition to all the uncompressing, it's going to have to scan a fair bit into the archive each time to extract each file.  One of tar's biggest drawbacks is that there is no table of contents at the beginning.  There's no efficient way to get information about all the files in the archive and only read that portion of the file.  It essentially has to read all of the file up to the thing you're extracting every time; it can't just jump to a filename's location right away.\nThe easiest thing you can do to speed this up would be to uncompress the file first (gunzip file.tar.gz) and then work on the .tar file.  That might help enough by itself.  It's still going to loop through the entire archive N times, though.\nIf you really want this to be efficient, your only option is to completely extract everything in the archive before processing it.  Since your problem is speed, I suspect this is a giant file that you don't want to extract first, but if you can, this will speed things up a lot:\ntar zxf file.tar.gz\nfor f in hopefullySomeSubdir/*; do\n  grep -l \"string\" $f\ndone\n\nNote that grep -l prints the name of any matching file, quits after the first match, and is silent if there's no match.  That alone will speed up the grepping portion of your command, so even if you don't have the space to extract the entire archive, grep -l will help.  If the files are huge, it will help a lot.\n", "\nFor starters, you could start more than one process:\ntar -ztf file.tar.gz | while read FILENAME\ndo\n        (if tar -zxf file.tar.gz \"$FILENAME\" -O | grep -l \"string\"\n        then\n                echo \"$FILENAME contains string\"\n        fi) &\ndone\n\nThe ( ... ) & creates a new detached (read: the parent shell does not wait for the child)\nprocess.\nAfter that, you should optimize the extracting of your archive. The read is no problem, \nas the OS should have cached the file access already. However, tar needs to unpack\nthe archive every time the loop runs, which can be slow. Unpacking the archive once\nand iterating over the result may help here:\nlocal tempPath=`tempfile`\nmkdir $tempPath && tar -zxf file.tar.gz -C $tempPath &&\nfind $tempPath -type f | while read FILENAME\ndo\n        (if grep -l \"string\" \"$FILENAME\"\n        then\n                echo \"$FILENAME contains string\"\n        fi) &\ndone && rm -r $tempPath\n\nfind is used here, to get a list of files in the target directory of tar, which we're iterating over, for each file searching for a string.\nEdit: Use grep -l to speed up things, as Jim pointed out. From man grep:\n   -l, --files-with-matches\n          Suppress normal output; instead print the name of each input file from which output would\n          normally have been printed.  The scanning will stop on the first match.  (-l is specified\n          by POSIX.)\n\n", "\n\nAm trying to grep pattern from dozen files .tar.gz but its very slow\ntar -ztf file.tar.gz | while read FILENAME\ndo\n        if tar -zxf file.tar.gz \"$FILENAME\" -O | grep \"string\" > /dev/null\n        then\n                echo \"$FILENAME contains string\"\n        fi\ndone\n\n\nThat's actually very easy with ugrep option -z:\n-z, --decompress\n        Decompress files to search, when compressed.  Archives (.cpio,\n        .pax, .tar, and .zip) and compressed archives (e.g. .taz, .tgz,\n        .tpz, .tbz, .tbz2, .tb2, .tz2, .tlz, and .txz) are searched and\n        matching pathnames of files in archives are output in braces.  If\n        -g, -O, -M, or -t is specified, searches files within archives\n        whose name matches globs, matches file name extensions, matches\n        file signature magic bytes, or matches file types, respectively.\n        Supported compression formats: gzip (.gz), compress (.Z), zip,\n        bzip2 (requires suffix .bz, .bz2, .bzip2, .tbz, .tbz2, .tb2, .tz2),\n        lzma and xz (requires suffix .lzma, .tlz, .xz, .txz).\n\nWhich requires just one command to search file.tar.gz as follows:\nugrep -z \"string\" file.tar.gz\n\nThis greps each of the archived files to display matches. Archived filenames are shown in braces to distinguish them from ordinary filenames. For example:\n$ ugrep -z \"Hello\" archive.tgz\n{Hello.bat}:echo \"Hello World!\"\nBinary file archive.tgz{Hello.class} matches\n{Hello.java}:public class Hello // prints a Hello World! greeting\n{Hello.java}:  { System.out.println(\"Hello World!\");\n{Hello.pdf}:(Hello)\n{Hello.sh}:echo \"Hello World!\"\n{Hello.txt}:Hello\n\nIf you just want the file names, use option -l (--files-with-matches) and customize the filename output with option --format=\"%z%~\" to get rid of the braces:\n$ ugrep -z Hello -l --format=\"%z%~\" archive.tgz\nHello.bat\nHello.class\nHello.java\nHello.pdf\nHello.sh\nHello.txt\n\n", "\nAll of the code above was really helpful, but none of it quite answered my own need: grep all *.tar.gz files in the current directory to find a pattern that is specified as an argument in a reusable script to output:\n\nThe name of both the archive file and the extracted file\nThe line number where the pattern was found\nThe contents of the matching line\n\nIt's what I was really hoping that zgrep could do for me and it just can't.\nHere's my solution:\npattern=$1\nfor f in *.tar.gz; do\n     echo \"$f:\"\n     tar -xzf \"$f\" --to-command 'grep --label=\"`basename $TAR_FILENAME`\" -Hin '\"$pattern ; true\";\ndone\n\nYou can also replace the tar line with the following if you'd like to test that all variables are expanding properly with a basic echo statement:\ntar -xzf \"$f\" --to-command 'echo \"f:`basename $TAR_FILENAME` s:'\"$pattern\\\"\"\n\nLet me explain what's going on.  Hopefully, the for loop and the echo of the archive filename in question is obvious.\ntar -xzf: x extract, z filter through gzip, f based on the following archive file...\n\"$f\": The archive file provided by the for loop (such as what you'd get by doing an ls) in double-quotes to allow the variable to expand and ensure that the script is not broken by any file names with spaces, etc.\n--to-command: Pass the output of the tar command to another command rather than actually extracting files to the filesystem.  Everything after this specifies what the command is (grep) and what arguments we're passing to that command.\nLet's break that part down by itself, since it's the \"secret sauce\" here.\n'grep --label=\"`basename $TAR_FILENAME`\" -Hin '\"$pattern ; true\"\n\nFirst, we use a single-quote to start this chunk so that the executed sub-command (basename $TAR_FILENAME) is not immediately expanded/resolved.  More on that in a moment.\ngrep: The command to be run on the (not actually) extracted files\n--label=: The label to prepend the results, the value of which is enclosed in double-quotes because we do want to have the grep command resolve the $TAR_FILENAME environment variable passed in by the tar command.\nbasename $TAR_FILENAME: Runs as a command (surrounded by backticks) and removes directory path and outputs only the name of the file\n-Hin: H Display filename (provided by the label), i Case insensitive search, n Display line number of match\nThen we \"end\" the first part of the command string with a single quote and start up the next part with a double quote so that the $pattern, passed in as the first argument, can be resolved.\nRealizing which quotes I needed to use where was the part that tripped me up the longest.  Hopefully, this all makes sense to you and helps someone else out.  Also, I hope I can find this in a year when I need it again (and I've forgotten about the script I made for it already!)\n\nAnd it's been a bit a couple of weeks since I wrote the above and it's still super useful... but it wasn't quite good enough as files have piled up and searching for things has gotten more messy.  I needed a way to limit what I looked at by the date of the file (only looking at more recent files).  So here's that code.  Hopefully it's fairly self-explanatory.\nif [ -z \"$1\" ]; then\n    echo \"Look within all tar.gz files for a string pattern, optionally only in recent files\"\n    echo \"Usage: targrep <string to search for> [start date]\"\nfi\npattern=$1\nstartdatein=$2\nstartdate=$(date -d \"$startdatein\" +%s)\nfor f in *.tar.gz; do\n    filedate=$(date -r \"$f\" +%s)\n    if [[ -z \"$startdatein\" ]] || [[ $filedate -ge $startdate ]]; then\n        echo \"$f:\"\n        tar -xzf \"$f\" --to-command 'grep --label=\"`basename $TAR_FILENAME`\" -Hin '\"$pattern ; true\"\n    fi\ndone\n\n\nAnd I can't stop tweaking this thing.  I added an argument to filter by the name of the output files in the tar file.  Wildcards work, too.\nUsage:\ntargrep.sh [-d <start date>] [-f <filename to include>] <string to search for>\nExample:\ntargrep.sh -d \"1/1/2019\" -f \"*vehicle_models.csv\" ford\nwhile getopts \"d:f:\" opt; do\n    case $opt in\n            d) startdatein=$OPTARG;;\n            f) targetfile=$OPTARG;;\n    esac\ndone\nshift \"$((OPTIND-1))\" # Discard options and bring forward remaining arguments\npattern=$1\n\necho \"Searching for: $pattern\"\nif [[ -n $targetfile ]]; then\n    echo \"in filenames:  $targetfile\"\nfi\n\nstartdate=$(date -d \"$startdatein\" +%s)\nfor f in *.tar.gz; do\n    filedate=$(date -r \"$f\" +%s)\n    if [[ -z \"$startdatein\" ]] || [[ $filedate -ge $startdate ]]; then\n            echo \"$f:\"\n            if [[ -z \"$targetfile\" ]]; then\n                    tar -xzf \"$f\" --to-command 'grep --label=\"`basename $TAR_FILENAME`\" -Hin '\"$pattern ; true\"\n            else\n                    tar -xzf \"$f\" --no-anchored \"$targetfile\" --to-command 'grep --label=\"`basename $TAR_FILENAME`\" -Hin '\"$pattern ; true\"\n            fi\n    fi\ndone\n\n", "\nzgrep works fine for me, only if all files inside is plain text.\nit looks nothing works if the tgz file contains gzip files.\n", "\nYou can mount the TAR archive with ratarmount and then simply search for the pattern in the mounted view:\npip install --user ratarmount\nratarmount large-archive.tar mountpoint\ngrep -r '<pattern>' mountpoint/\n\nThis is much faster than iterating over each file and piping it to grep separately, especially for compressed TARs. Here are benchmark results in seconds for a 55 MiB uncompressed and 42 MiB compressed TAR archive containing 40 files:\n\n\n\n\nCompression\nRatarmount\nBash Loop over tar -O\n\n\n\n\nnone\n0.31 +- 0.01\n0.55 +- 0.02\n\n\ngzip\n1.1 +- 0.1\n13.5 +- 0.1\n\n\nbzip2\n1.2 +- 0.1\n97.8 +- 0.2\n\n\n\n\nOf course, these results are highly dependent on the archive size and how many files the archive contains. These test examples are pretty small because I didn't want to wait too long. But, they already exemplify the problem well enough. The more files there are, the longer it takes for tar -O to jump to the correct file. And for compressed archives, it will be quadratically slower the larger the archive size is because everything before the requested file has to be decompressed and each file is requested separately. Both of these problems are solved by ratarmount.\n\nThis is the code for benchmarking:\nfunction checkFilesWithRatarmount()\n{\n    local pattern=$1\n    local archive=$2\n    ratarmount \"$archive\" \"$archive.mountpoint\"\n    'grep' -r -l \"$pattern\" \"$archive.mountpoint/\"\n}\n\nfunction checkEachFileViaStdOut()\n{\n    local pattern=$1\n    local archive=$2\n    tar --list --file \"$archive\" | while read -r file; do\n        if tar -x --file \"$archive\" -O -- \"$file\" | grep -q \"$pattern\"; then\n            echo \"Found pattern in: $file\"\n        fi\n    done\n}\n\nfunction createSampleTar()\n{\n    for i in $( seq 40 ); do \n        head -c $(( 1024 * 1024 )) /dev/urandom | base64 > $i.dat\n    done\n    tar -czf \"$1\" [0-9]*.dat\n}\n\ncreateSampleTar myarchive.tar.gz\ntime checkEachFileViaStdOut ABCD myarchive.tar.gz\ntime checkFilesWithRatarmount ABCD myarchive.tar.gz\nsleep 0.5s\nfusermount -u myarchive.tar.gz.mountpoint\n\n", "\nI adopted the @Jotne solution and works perfectly! For example for mongodb server in my NAS\n#! /bin/bash\n\ncase \"$(pidof mongod | wc -w)\" in\n\n0)  echo \"Restarting mongod:\"\n    mongod --config mongodb.conf\n    ;;\n1)  echo \"mongod already running\"\n    ;;\nesac\n\n", "\n\nPrograms to monitor if a process on a system is running. \n\nScript is stored in crontab and runs once every minute.\nThis works with if process is not running or process is running multiple times:\n#! /bin/bash\n\ncase \"$(pidof amadeus.x86 | wc -l)\" in\n\n0)  echo \"Restarting Amadeus:     $(date)\" >> /var/log/amadeus.txt\n    /etc/amadeus/amadeus.x86 &\n    ;;\n1)  # all ok\n    ;;\n*)  echo \"Removed double Amadeus: $(date)\" >> /var/log/amadeus.txt\n    kill $(pidof amadeus.x86 | awk '{print $1}')\n    ;;\nesac\n\n0 If process is not found, restart it.\n1 If process is found, all ok.\n* If process running 2 or more, kill the last.\n\nA simpler version.  This just test if process is running, and if not restart it.\nIt just tests the exit flag $? from the pidof program.  It will be 0 of process is running and 1 if not.\n#!/bin/bash\npidof  amadeus.x86 >/dev/null\nif [[ $? -ne 0 ]] ; then\n        echo \"Restarting Amadeus:     $(date)\" >> /var/log/amadeus.txt\n        /etc/amadeus/amadeus.x86 &\nfi\n\n\nAnd at last, a one liner\npidof amadeus.x86 >/dev/null ; [[ $? -ne 0 ]] && echo \"Restarting Amadeus:     $(date)\" >> /var/log/amadeus.txt && /etc/amadeus/amadeus.x86 &\n\nThis can then be used in crontab to run every minute like this:\n* * * * * pidof amadeus.x86 >/dev/null ; [[ $? -ne 0 ]] && echo \"Restarting Amadeus:     $(date)\" >> /var/log/amadeus.txt && /etc/amadeus/amadeus.x86 &\n\ncccam oscam\n", "\nI have adopted your script for my situation Jotne.\n#! /bin/bash\n\nlogfile=\"/var/oscamlog/oscam1check.log\"\n\ncase \"$(pidof oscam1 | wc -w)\" in\n\n0)  echo \"oscam1 not running, restarting oscam1:     $(date)\" >> $logfile\n    /usr/local/bin/oscam1 -b -c /usr/local/etc/oscam1 -t /usr/local/tmp.oscam1 &\n    ;;\n2)  echo \"oscam1 running, all OK:     $(date)\" >> $logfile\n    ;;\n*)  echo \"multiple instances of oscam1 running. Stopping & restarting oscam1:     $(date)\" >> $logfile\n    kill $(pidof oscam1 | awk '{print $1}')\n    ;;\nesac\n\nWhile I was testing, I ran into a problem..\nI started 3 extra process's of oscam1 with this line:\n/usr/local/bin/oscam1 -b -c /usr/local/etc/oscam1 -t /usr/local/tmp.oscam1\nwhich left me with 8 process for oscam1.  the problem is this..\nWhen I run the script, It only kills 2 process's at a time, so I would have to run it 3 times to get it down to 2 process..\nOther than killall -9 oscam1 followed by /usr/local/bin/oscam1 -b -c /usr/local/etc/oscam1 -t /usr/local/tmp.oscam1, in *)is there any better way to killall apart from the original process? So there would be zero downtime?\n", "\nIf you changed awk '{print $1}' to '{ $1=\"\"; print $0}' you will get all processes except for the first as a result. It will start with the field separator (a space generally) but I don't recall killall caring. So:\n#! /bin/bash\n\nlogfile=\"/var/oscamlog/oscam1check.log\"\n\ncase \"$(pidof oscam1 | wc -w)\" in\n\n0)  echo \"oscam1 not running, restarting oscam1:     $(date)\" >> $logfile\n    /usr/local/bin/oscam1 -b -c /usr/local/etc/oscam1 -t /usr/local/tmp.oscam1 &\n    ;;\n2)  echo \"oscam1 running, all OK:     $(date)\" >> $logfile\n    ;;\n*)  echo \"multiple instances of oscam1 running. Stopping & restarting oscam1:     $(date)\" >> $logfile\n    kill $(pidof oscam1 | awk '{ $1=\"\"; print $0}')\n    ;;\nesac\n\nIt is worth noting that the pidof route seems to work fine for commands that have no spaces, but you would probably want to go back to a ps-based string if you were looking for, say, a python script named myscript that showed up under ps like \nroot     22415 54.0  0.4  89116 79076 pts/1    S    16:40   0:00 /usr/bin/python /usr/bin/myscript\nJust an FYI\n", "\nThe 'pidof' command will not display pids of shell/perl/python scripts. So to find the process id\u2019s of my Perl script I had to use the -x option i.e. 'pidof -x perlscriptname'\n", "\nI cannot get case to work at all.\nHeres what I have: \n#! /bin/bash\n\nlogfile=\"/home/name/public_html/cgi-bin/check.log\"\n\ncase \"$(pidof -x script.pl | wc -w)\" in\n\n0)  echo \"script not running, Restarting script:     $(date)\" >> $logfile\n#  ./restart-script.sh\n;;\n1)  echo \"script Running:     $(date)\" >> $logfile\n;;\n*)  echo \"Removed duplicate instances of script: $(date)\" >> $logfile\n #   kill $(pidof -x ./script.pl | awk '{ $1=\"\"; print $0}')\n;;\nesac\n\nrem the case action commands for now just to test the script. the above pidof -x command is returning '1', the case statement is returning the results for '0'.\nAnyone have any idea where I'm going wrong?\nSolved it by adding the following to my BIN/BASH Script:\n    PATH=$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n", "\nIn case you're looking for a more modern way to check to see if a service is running (this will not work for just any old process), then systemctl might be what you're looking for.\nHere's the basic command:\nsystemctl show --property=ActiveState your_service_here\n\nWhich will yield very simple output (one of the following two lines will appear depending on whether the service is running or not running):\nActiveState=active\nActiveState=inactive\n\nAnd if you'd like to know all of the properties you can get:\nsystemctl show --all your_service_here\n\nIf you prefer that alphabetized:\nsystemctl show --all your_service_here | sort\n\nAnd the full code to act on it:\nservice=$1\nresult=`systemctl show --property=ActiveState $service`\nif [[ \"$result\" == 'ActiveState=active' ]]; then\n    echo \"$service is running\" # Do something here\nelse\n    echo \"$service is not running\" # Do something else here\nfi \n\n", "\nYou may use this gnu-awk solution:\nawk -v RS='{[^}]*}\\n' '{ORS = (RT ~ /:\\s*main/ ? \"\" : RT)} 1' file\n\n{\n  name: test\n  phase: dev\n}\n\nHere:\n\nRS='{[^}]*}\\n' will set a {...}\\n text as record separator.\nORS = (RT ~ /:\\s*main/ ? \"\" : RT) will set ORS to an empty string when we find : main in the RT otherwise ORS is set to RT.\n\n", "\nThis might work for you (GNU sed):\nsed 'N;/\\n.*\\bmain\\b/{N;N;d};P;D' file\n\nOpen a two line window and if the second line contains the word main, append the next two lines and then delete the four lines in the pattern space.\nIf the second line does not contain the word main, print/delete the first and repeat.\n", "\nUsing any awk:\n$ awk '{rec=rec $0 ORS} /^}$/{if (rec !~ /main/) printf \"%s\", rec; rec=\"\"}' file\n{\n  name: test\n  phase: dev\n}\n\nThe above will work even if your data contains { or } in other contexts such as inside strings and no matter how many lines are in each }-terminated block. It assumes you're OK with substring matches on main (e.g. mainstay or germain), just like the grep command in your question would be - if that's wrong then just tighten the regexp to be more exclusive, e.g. /name: *main\\n/.\n", "\nWith awk:\nawk -F_ 'length($1)==3{print $1}'\n\n-F_ tells awk to split the input lines by _. length($1) == 3 checks whether the first fields (the name) is 3 characters long and {print $1} prints the name in that case.\n", "\nThe following command can be tried out :\ngrep -w \"...\" file.txt\n\n", "\nHere is the example using GNU grep:\ngrep -Pzo '_name.*\\n.*_description'\n\n\n-z/--null-data Treat the input as a set of lines, each terminated by a zero byte (the ASCII NUL character) instead of a newline.\n\nWhich has the effect of treating the whole file as one large line.\nSee -z description on grep's manual and also common question no 14 on grep's manual usage page\n", "\nWhy don't you go for awk:\nawk '/Start pattern/,/End pattern/' filename\n\n", "\nSo I discovered pcregrep which stands for Perl Compatible Regular Expressions GREP.\n\nthe -M option makes it possible to search for patterns that span line boundaries.\n\nFor example, you need to find files where the '_name' variable is followed on the next line by the '_description' variable:\nfind . -iname '*.py' | xargs pcregrep -M '_name.*\\n.*_description'\n\nTip: you need to include the line break character in your pattern. Depending on your platform, it could be '\\n', \\r', '\\r\\n', ...\n", "\ngrep -P also uses libpcre, but is much more widely installed. To find a complete title section of an html document, even if it spans multiple lines, you can use this:\ngrep -P '(?s)<title>.*</title>' example.html\n\nSince the PCRE project implements to the perl standard, use the perl documentation for reference:\n\nhttp://perldoc.perl.org/perlre.html#Modifiers\nhttp://perldoc.perl.org/perlre.html#Extended-Patterns\n\n", "\nHere is a more useful example:\npcregrep -Mi \"<title>(.*\\n){0,5}</title>\" afile.html\n\nIt searches the title tag in a html file even if it spans up to 5 lines.\nHere is an example of unlimited lines:\npcregrep -Mi \"(?s)<title>.*</title>\" example.html \n\n", "\nWith silver searcher:\nag 'abc.*(\\n|.)*efg'\n\nSpeed optimizations of silver searcher could possibly shine here.\n", "\nThis answer might be useful:\nRegex (grep) for multi-line search needed\nTo find recursively you can use flags -R (recursive) and --include (GLOB pattern). See:\nUse grep --exclude/--include syntax to not grep through certain files\n", "\n@Marcin:\nawk example non-greedy:\nawk '{if ($0 ~ /Start pattern/) {triggered=1;}if (triggered) {print; if ($0 ~ /End pattern/) { exit;}}}' filename\n\n", "\nYou can use the grep alternative sift here (disclaimer: I am the author).\nIt support multiline matching and limiting the search to specific file types out of the box:\nsift -m --files '*.py' 'YOUR_PATTERN'\n\n(search all *.py files for the specified multiline regex pattern)\nIt is available for all major operating systems. Take a look at the samples page to see how it can be used to  to extract multiline values from an XML file.\n", "\nperl -ne 'print if (/begin pattern/../end pattern/)' filename\n\n", "\nUsing ex/vi editor and globstar option (syntax similar to awk and sed):\nex +\"/string1/,/string3/p\" -R -scq! file.txt\n\nwhere aaa is your starting point, and bbb is your ending text.\nTo search recursively, try:\nex +\"/aaa/,/bbb/p\" -scq! **/*.py\n\nNote: To enable ** syntax, run shopt -s globstar (Bash 4 or zsh).\n", "\nI believe the following should work and has the advantage of only using extended regular expressions without the need to install an extra tool like pcregrep if you don\u2019t have it yet or don\u2019t have the -P option to grep available (eg. macOS):\negrep -irzo \u201c.*aaa(.*\\s.*){1,}.*bbb.*\" path_to_filenames\nCaveat emptor: this does some slight disadvantages:\n\nit will find the largest selection of lines from the first aaa to the last bbb in each file, unless...\nthere are several repetitions of the aaa [stuff] bbb pattern in each file.\n\n"], "rejected": ["\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nA simpler way would be to filter your results using \"grep -v\".\ngrep -i needle -R * | grep -v node_modules\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYou can use find and the parameter -executable that is basically what you want.\nThe manpages says:\n   -executable\n          Matches files which are executable and directories which are searchable (in a file name resolution sense).  This takes into  account  access control lists and other permissions artefacts which the -perm test ignores.  This test makes use of the access(2) system call, and so can be fooled by NFS servers which do UID mapping (or root-squashing), since many systems implement access(2) in the client's kernel and so  cannot make  use  of  the  UID mapping information held on the server.  Because this test is based only on the result of the access(2) system call, there is no guarantee that a file for which this test succeeds can actually be executed.\n\nThis is a result of what you want:\n# find /bin  -executable -type f | grep 'dmesg'\n/bin/dmesg\n\n", "\nYour issue is that .* will match characters from the first IT1 with EA to the last REF with BAR. You need to ensure the match doesn't go past the next IT1, which you can do by replacing .* with a tempered greedy token (?:(?!@IT1).)*:\nIT1[^@]*EA[^@]*@(?:(?!@IT1).)*REF[^@]*BAR[^@]*@\n\nThis will only match from an IT1 to its corresponding REF.\nRegex demo on regex101\n", "\ngrep by default returns the entire line when a match is found on a given input line.\nWhile option -o restricts the output to only that part of the line that the regex matched, that is still not enough in this case, because you want a substring of that match.\nHowever, since you're on Linux, you can use GNU grep's -P option (for support of PCREs, Perl-compatible regular expression), which allows extracting a submatch by way of features such as \\K (drop everything matched so far) and (?=...) (a look-ahead assertion that does not contribute to the match):\n$ grep -Po  \"[a-zA-Z]\\(.*\\)\\K[a-z]+(?=:)\" <<'EOF'\nMsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nEOF\nnoop  # output\n\n\nOptional background information:\nEd Morton points out (in a since-deleted comment) that GNU grep's man page still calls the -P option \"highly experimental\" that may \"warn of unimplemented features\", but the option has been around for years, and in practice I have yet to see a warning or a performance problem - YMMV.  \nIn the case at hand, the above command even outperforms sed and awk solutions - see NeronLeVelu's helpful performance comparison.\nThe interesting article Ed points to discusses a potential performance problem that can surface with regex engines such as used by grep -P (via the PCRE library), Perl itself, and many other widely used (and mature) regex engines, such as in Python, Ruby, and PHP: \n\nIn short: the recursive backtracking algorithm employed by these engines can result in severe performance degradation with \"pathological\" regexes that string together long sequences of subexpressions with variable-length quantifiers, such as (a longer version of) a?a?a?a?aaaa to match aaaa.\nThe article argues that backtracking is only truly required when a regex contains backreferences, and that a different, much faster algorithm should be employed in their absence. \n\n", "\ngrep by default returns the entire line when a match is found on a given input line.\nWhile option -o restricts the output to only that part of the line that the regex matched, that is still not enough in this case, because you want a substring of that match.\nHowever, since you're on Linux, you can use GNU grep's -P option (for support of PCREs, Perl-compatible regular expression), which allows extracting a submatch by way of features such as \\K (drop everything matched so far) and (?=...) (a look-ahead assertion that does not contribute to the match):\n$ grep -Po  \"[a-zA-Z]\\(.*\\)\\K[a-z]+(?=:)\" <<'EOF'\nMsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nEOF\nnoop  # output\n\n\nOptional background information:\nEd Morton points out (in a since-deleted comment) that GNU grep's man page still calls the -P option \"highly experimental\" that may \"warn of unimplemented features\", but the option has been around for years, and in practice I have yet to see a warning or a performance problem - YMMV.  \nIn the case at hand, the above command even outperforms sed and awk solutions - see NeronLeVelu's helpful performance comparison.\nThe interesting article Ed points to discusses a potential performance problem that can surface with regex engines such as used by grep -P (via the PCRE library), Perl itself, and many other widely used (and mature) regex engines, such as in Python, Ruby, and PHP: \n\nIn short: the recursive backtracking algorithm employed by these engines can result in severe performance degradation with \"pathological\" regexes that string together long sequences of subexpressions with variable-length quantifiers, such as (a longer version of) a?a?a?a?aaaa to match aaaa.\nThe article argues that backtracking is only truly required when a regex contains backreferences, and that a different, much faster algorithm should be employed in their absence. \n\n", "\ngrep pattern filename | grep pattern | grep pattern | grep pattern ......\n", "\ngrep pattern filename | grep pattern | grep pattern | grep pattern ......\n", "\ngrep pattern filename | grep pattern | grep pattern | grep pattern ......\n", "\ngrep pattern filename | grep pattern | grep pattern | grep pattern ......\n", "\ngrep pattern filename | grep pattern | grep pattern | grep pattern ......\n", "\nI used for my csv, I hope it should help you too.\n\n$ bash\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'\noutput: list of files\nLet's say, you want the count also of files and not just list them\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'|wc -l\noutput: 42\nLet's say you want size of files\nbash-4.4$ ls -ls\n\n", "\nI used for my csv, I hope it should help you too.\n\n$ bash\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'\noutput: list of files\nLet's say, you want the count also of files and not just list them\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'|wc -l\noutput: 42\nLet's say you want size of files\nbash-4.4$ ls -ls\n\n", "\nI used for my csv, I hope it should help you too.\n\n$ bash\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'\noutput: list of files\nLet's say, you want the count also of files and not just list them\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'|wc -l\noutput: 42\nLet's say you want size of files\nbash-4.4$ ls -ls\n\n", "\nI used for my csv, I hope it should help you too.\n\n$ bash\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'\noutput: list of files\nLet's say, you want the count also of files and not just list them\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'|wc -l\noutput: 42\nLet's say you want size of files\nbash-4.4$ ls -ls\n\n", "\nI used for my csv, I hope it should help you too.\n\n$ bash\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'\noutput: list of files\nLet's say, you want the count also of files and not just list them\nbash-4.4$ ls *.csv | grep '^[Kk][Ss]*'|wc -l\noutput: 42\nLet's say you want size of files\nbash-4.4$ ls -ls\n\n", "\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", "\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", "\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", "\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", "\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIn my case the tarballs have a lot of tiny files and I want to know what archived file inside the tarball matches. zgrep is fast (less than one second) but doesn't tell me what file in the tarball matched, and tar --to-command grep gives me the info that I want, but is much, much slower (many minutes)1.\nSo I went the other direction and had zgrep tell me the byte offsets of the matches in the original tar and put that together with the list of file offsets in the original tar to find the matching archived files.\n#!/bin/bash\nset -e\nset -o pipefail\n\nfunction tar_offsets() {\n\n    # Get the byte offsets of all the files in a given tarball \n    # based on https://stackoverflow.com/a/49865044/60422\n\n    [ $# -eq 1 ]\n\n    tar -tvf \"$1\" -R | awk '\n    BEGIN{\n      getline;\n      f=$8;\n      s=$5;\n    }\n    {\n      offset = int($2) * 512 - and((s+511), compl(512)+1)\n      print offset,s,f;\n      f=$8;\n      s=$5;\n    }'\n\n}\n\nfunction tar_byte_offsets_to_files() {\n    [ $# -eq 1 ]\n\n    # Convert the search results of a tarball with byte offsets \n    # to search results with archived file name and offset, using\n    # the provided tar_offsets output (single pass, suitable for\n    # process substitution)\n\n    offsets_file=\"$1\"\n\n    prev_offset=0\n    prev_offset_filename=\"\"\n\n    IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n    while IFS=':' read -r search_result_offset match_text\n    do\n        while [ $last_offset -lt $search_result_offset ]; do\n            prev_offset=$last_offset\n            prev_offset_filename=\"$last_offset_filename\"\n\n            IFS=' ' read -r last_offset last_len last_offset_filename < \"$offsets_file\"\n\n            # offsets increasing safeguard\n            [ $prev_offset -le $last_offset ]\n        done\n\n        # now last offset is the first file strictly after search result offset so prev offset is\n        # the one at or before it, and must be the one it is in\n\n        result_file_offset=$(( $search_result_offset - $prev_offset ))\n\n        echo \"$prev_offset_filename:$result_file_offset:$match_text\"\n    done\n}\n\n# Putting it together e.g.\nzgrep -a --byte-offset \"your search here\" some.tgz | tar_byte_offsets_to_files <(tar_offsets some.tgz)\n\n\n\n1 I'm running this in Git for Windows' minimal MSYS2 fork unixy environment, so it's possible that the launch overhead of grep is much much higher than on any kind of real Unix machine and would make `tar --to-command grep` good enough there; benchmark solutions for your own needs and platform situation before selecting.\n    ", "\nIf you are using CentOS, no need to write a script and set cron job. Here is one of the smartest ways to ensure systemd services restart on failure.\nMake following changes to /usr/lib/systemd/system/mariadb.service\nThen under the [Service] section in the file, add the following 2 lines:\nRestart=always\nRestartSec=3\n\nAfter saving the file we need to reload the daemon configurations to ensure systemd is aware of the new file\nsystemctl daemon-reload\n\nRead the following link for the complete steps -\nhttps://jonarcher.info/2015/08/ensure-systemd-services-restart-on-failure/\n", "\nIf you are using CentOS, no need to write a script and set cron job. Here is one of the smartest ways to ensure systemd services restart on failure.\nMake following changes to /usr/lib/systemd/system/mariadb.service\nThen under the [Service] section in the file, add the following 2 lines:\nRestart=always\nRestartSec=3\n\nAfter saving the file we need to reload the daemon configurations to ensure systemd is aware of the new file\nsystemctl daemon-reload\n\nRead the following link for the complete steps -\nhttps://jonarcher.info/2015/08/ensure-systemd-services-restart-on-failure/\n", "\nIf you are using CentOS, no need to write a script and set cron job. Here is one of the smartest ways to ensure systemd services restart on failure.\nMake following changes to /usr/lib/systemd/system/mariadb.service\nThen under the [Service] section in the file, add the following 2 lines:\nRestart=always\nRestartSec=3\n\nAfter saving the file we need to reload the daemon configurations to ensure systemd is aware of the new file\nsystemctl daemon-reload\n\nRead the following link for the complete steps -\nhttps://jonarcher.info/2015/08/ensure-systemd-services-restart-on-failure/\n", "\nIf you are using CentOS, no need to write a script and set cron job. Here is one of the smartest ways to ensure systemd services restart on failure.\nMake following changes to /usr/lib/systemd/system/mariadb.service\nThen under the [Service] section in the file, add the following 2 lines:\nRestart=always\nRestartSec=3\n\nAfter saving the file we need to reload the daemon configurations to ensure systemd is aware of the new file\nsystemctl daemon-reload\n\nRead the following link for the complete steps -\nhttps://jonarcher.info/2015/08/ensure-systemd-services-restart-on-failure/\n", "\nIf you are using CentOS, no need to write a script and set cron job. Here is one of the smartest ways to ensure systemd services restart on failure.\nMake following changes to /usr/lib/systemd/system/mariadb.service\nThen under the [Service] section in the file, add the following 2 lines:\nRestart=always\nRestartSec=3\n\nAfter saving the file we need to reload the daemon configurations to ensure systemd is aware of the new file\nsystemctl daemon-reload\n\nRead the following link for the complete steps -\nhttps://jonarcher.info/2015/08/ensure-systemd-services-restart-on-failure/\n", "\nIf you are using CentOS, no need to write a script and set cron job. Here is one of the smartest ways to ensure systemd services restart on failure.\nMake following changes to /usr/lib/systemd/system/mariadb.service\nThen under the [Service] section in the file, add the following 2 lines:\nRestart=always\nRestartSec=3\n\nAfter saving the file we need to reload the daemon configurations to ensure systemd is aware of the new file\nsystemctl daemon-reload\n\nRead the following link for the complete steps -\nhttps://jonarcher.info/2015/08/ensure-systemd-services-restart-on-failure/\n", "\nIf you are using CentOS, no need to write a script and set cron job. Here is one of the smartest ways to ensure systemd services restart on failure.\nMake following changes to /usr/lib/systemd/system/mariadb.service\nThen under the [Service] section in the file, add the following 2 lines:\nRestart=always\nRestartSec=3\n\nAfter saving the file we need to reload the daemon configurations to ensure systemd is aware of the new file\nsystemctl daemon-reload\n\nRead the following link for the complete steps -\nhttps://jonarcher.info/2015/08/ensure-systemd-services-restart-on-failure/\n", "\ned can easily make such relative changes:\nprintf '/main/-1;+3d\\nw\\nq\\n' | ed -s sourcefile\n\nor\ned -s sourcefile <<'EOD'\n/main/-1;+3d\nw\nq\nEOD\n\n\nfind line matching regex and move to line above it\ndelete that and next 3 lines\nsave changes\nquit\n\n", "\ned can easily make such relative changes:\nprintf '/main/-1;+3d\\nw\\nq\\n' | ed -s sourcefile\n\nor\ned -s sourcefile <<'EOD'\n/main/-1;+3d\nw\nq\nEOD\n\n\nfind line matching regex and move to line above it\ndelete that and next 3 lines\nsave changes\nquit\n\n", "\ned can easily make such relative changes:\nprintf '/main/-1;+3d\\nw\\nq\\n' | ed -s sourcefile\n\nor\ned -s sourcefile <<'EOD'\n/main/-1;+3d\nw\nq\nEOD\n\n\nfind line matching regex and move to line above it\ndelete that and next 3 lines\nsave changes\nquit\n\n", "\nYou need to match the _ after the first name.\ngrep -E \"[A-Z][a-z]{2}_\"\n\n", "\nYou need to match the _ after the first name.\ngrep -E \"[A-Z][a-z]{2}_\"\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n", "\nAs Amit's answer earlier, you can use awk to search for multiple lines. In case you need to print the line number, use the following:\nawk '/Start pattern/,/End pattern/ {print NR \":\" $0}' filename\n\n"]}