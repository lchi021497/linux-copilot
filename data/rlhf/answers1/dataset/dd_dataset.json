{"prompt": ["Trying to save money on EBS snapshots, so the idea is to take manual copies of the file systems (using dd) and storing manually in S3 to lifecycle to IA and Glacier.The following works fine for smaller files (tested with 1GB), but on larger (~800GB), after around 40GB, everything slows to a crawl and never finishes sudo dd if=/dev/sdb bs=64M status=progress | aws s3 cp - s3://my-bucket/sdb_backup.img --sse AES256 --storage-class STANDARD_IA\nRunning this from an m4.4xlarge instance (16 vcpu, 64GB RAM)Not exactly sure why it's crawling to a halt, or whether this is the best way to solve this problem (manually storing file systems on s3 Infrequent Access storage class)Any thoughts?Thanks!!", "Trying to save money on EBS snapshots, so the idea is to take manual copies of the file systems (using dd) and storing manually in S3 to lifecycle to IA and Glacier.The following works fine for smaller files (tested with 1GB), but on larger (~800GB), after around 40GB, everything slows to a crawl and never finishes sudo dd if=/dev/sdb bs=64M status=progress | aws s3 cp - s3://my-bucket/sdb_backup.img --sse AES256 --storage-class STANDARD_IA\nRunning this from an m4.4xlarge instance (16 vcpu, 64GB RAM)Not exactly sure why it's crawling to a halt, or whether this is the best way to solve this problem (manually storing file systems on s3 Infrequent Access storage class)Any thoughts?Thanks!!", "Trying to save money on EBS snapshots, so the idea is to take manual copies of the file systems (using dd) and storing manually in S3 to lifecycle to IA and Glacier.The following works fine for smaller files (tested with 1GB), but on larger (~800GB), after around 40GB, everything slows to a crawl and never finishes sudo dd if=/dev/sdb bs=64M status=progress | aws s3 cp - s3://my-bucket/sdb_backup.img --sse AES256 --storage-class STANDARD_IA\nRunning this from an m4.4xlarge instance (16 vcpu, 64GB RAM)Not exactly sure why it's crawling to a halt, or whether this is the best way to solve this problem (manually storing file systems on s3 Infrequent Access storage class)Any thoughts?Thanks!!"], "chosen": ["\nIt is not a good idea because snapshots are incremental, so you'll spend more starting from the next few hand-made snapshots.\nIf you still want this way then consider multi-part upload (chunks up to 5GB).\n", "\nYou can use something like goofys to redirect output to S3. I've personally tested with files up to 1TB.\n", "\nFirst consider the multipart upload for large sizes.\nSecond use the compressed version,\ndd if=/dev/sdX | gzip -c | aws s3 cp - s3://bucket-name/desired_image_name.img\n\n"], "rejected": ["\nIf you wish to copy files to Amazon S3, the easiest method is to use the AWS Command-Line Interface (CLI):\naws s3 sync dir s3://my-bucket/dir\n\nAs an alternative to Standard-Infrequent Access, you could create a lifecycle policy on the S3 bucket to move files to Glacier. (This is worthwhile for long-term storage, but not for the short-term due to higher request charges.)\n", "\nIf you wish to copy files to Amazon S3, the easiest method is to use the AWS Command-Line Interface (CLI):\naws s3 sync dir s3://my-bucket/dir\n\nAs an alternative to Standard-Infrequent Access, you could create a lifecycle policy on the S3 bucket to move files to Glacier. (This is worthwhile for long-term storage, but not for the short-term due to higher request charges.)\n", "\nIf you wish to copy files to Amazon S3, the easiest method is to use the AWS Command-Line Interface (CLI):\naws s3 sync dir s3://my-bucket/dir\n\nAs an alternative to Standard-Infrequent Access, you could create a lifecycle policy on the S3 bucket to move files to Glacier. (This is worthwhile for long-term storage, but not for the short-term due to higher request charges.)\n"]}