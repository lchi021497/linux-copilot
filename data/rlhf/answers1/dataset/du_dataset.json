{"prompt": ["I am trying to find directories in a given size but it doesn't display. I am running the code on the desired directory and I rundu -h --maxdepth 1\nTo make sure that the directories have the sizes I want to find. But when I run:find . -maxdepth 1 -type d -size +5M -size -3M\nit doesn't display anything.", "I am trying to find directories in a given size but it doesn't display. I am running the code on the desired directory and I rundu -h --maxdepth 1\nTo make sure that the directories have the sizes I want to find. But when I run:find . -maxdepth 1 -type d -size +5M -size -3M\nit doesn't display anything.", "So i want to display the filename and the size of the file in MB using the \"du\" command in my bash script but it outputs the filepath for the fileI have used the command:du -s -BM /home/user/test/test_file_check\nBut my output is:0M      /home/user/test/test_file_check\nHow do i get rid of the /home/user/test path from the output?\nWould it be possible to use sed to remove the filepath?Is there alternatively another command better suited for this?", "I'm not familiar with bash scripting. Maybe this is a silly question. But I couldn't find the answer. I'm working on a bash script that mimics the behavior of the command ls -sh but that actually uses du -sh to get file and folder sizes. And it sorts the output. pretty much like du -sh* | sort -h with colors.#!/usr/bin/bash\n\nif [ \"$#\" = \"0\" ]\nthen\n    du -sh *|awk -f /path/to/color-ls.awk|sort -h\nelse\n    du -sh $@|awk -f /path/to/color-ls.awk|sort -h\nfi\nwhere ls-color.awk is:# color-ls.awk\nsize=$1;\nname=$2;\nfor (i=3; i<=NF; i++)\n{\n    tmp=(name \" \" $i);\n    name=tmp\n}\n# filename=($0 ~ /'/)? (\"\\\"\" name \"\\\"\"):(\"'\" name \"'\")\nfilename=(\"'\" name \"'\")\nprintf $1 \" \"\ncmd=(\"ls -d \" filename \" --color\")\nsystem(cmd)\nan awk script that uses ls --color to color the output of du -shMy scripts works fine with most file names even ones containing spaces. but it has some problems involving special characters that I didn't know how to fix.1. When run without arguments:It is interpreting any file name that contains single quotes causing an errorsh: 1: Syntax error: Unterminated quoted string\n2. When run with arguments:The same problem as without arguments. And it's interpreting a file name with spaces as two names.example: when used on a folder named VirtualBox VMs or when given * as an argument in my home directory here's it's output:du: cannot access 'VirtualBox': No such file or directory\ndu: cannot access 'VMs': No such file or directory\n3. What I want:I want the script to skip special characters and pass them as they are to du4. What I tried:I tried adding double quotes before and after each file nameparse(){\n    for arg in $@\n    do\n        printf \"\\\"$arg\\\"\\n\"\n    done\n}\nbut it didn't seem to work. du doesn't accept quotes appended to the file name.du: cannot access '\"VirtualBox': No such file or directory\ndu: cannot access 'VMs\"': No such file or directory\nAlso, replacing quotes with \\' doesn't help ether. maybe I'm just doing it wrong.# du -sh $(printf \"file'name\\n\" |sed \"s/'/\\\\\\'/g\")\ndu: cannot access 'file\\'\\''name': No such file or directory\n# ls file\\'name \n\"file'name\"\nSame goes for spacesdu: cannot access 'VirtualBox\\': No such file or directory\ndu: cannot access 'VMs': No such file or directory\n5. Extra:I'm trying to make the script works as normal ls -sh would work but with sorted output and with more accurate results when it comes to folders. but this script works like ls -sh -d when arguments are supplied to it. making lh Desktop shows the size of Desktop instead of the size of the individual files and folders inside Desktop. I believe this can be fixed with a loop that checks if each argument is a file or a folder and execute du -sh accordingly then sort.#!/usr/bin/bash\n\nif [ \"$#\" = \"0\" ]\nthen\n    du -sh *|awk -f /path/to/color-ls.awk|sort -h\nelse\n    for i in $@\n    do\n        if [[ -d \"$i\" ]]; then\n            du -sh $i/* |awk -f /path/to/color-ls.awk\n        else\n            du -sh \"$i\" |awk -f /path/to/color-ls.awk\n        fi\n    done|sort -h\nfi\n\nI'm hoping to find the optimal way to do it.Thanks in advance.", "We have a software package that performs tasks by assigning the batch of files a job number. Batches can have any number of files in them. The files are then stored in a directory structure similar to this:/asc/array1/.storage/10/10297/10297-Low-res.m4a\n...\n/asc/array1/.storage/3/3814/3814-preview.jpg\nThe filename is generated automatically. The directory in .storage is the thousandths digits of the file number.There is also a database which associates the job number and the file number with the client in question. Running a SQL query, I can list out the job number, client and the full path to the files. Example:213     sample-data     /asc/array1/.storage/10/10297/10297-Low-res.m4a\n...\n214     client-abc      /asc/array1/.storage/3/3814/3814-preview.jpg\nMy task is to calculate the total storage being used per client. So, I wrote a quick and dirty bash script to iterate over every single row and du the file, adding it to an associative array. I then plan to echo this out or produce a CSV file for ingest into PowerBI or some other tool. Is this the best way to handle this? Here is a copy of the script as it stands:#!/bin/sh\n\ndeclare -A clientArr\n\n# 1 == Job Num\n# 2 == Client\n# 3 == Path\nwhile read line; do\n    client=$(echo \"$line\" | awk '{ print $2 }')\n    path=$(echo \"$line\" | awk '{ print $3 }')\n\n    if [ -f \"$path\" ]; then\n        size=$(du -s \"$path\" | awk '{ print $1 }')\n        clientArr[$client]=$((${clientArr[$client]}+${size}))\n    fi\ndone < /tmp/pm_report.txt\n\nfor key in \"${!clientArr[@]}\"; do\n    echo \"$key,${clientArr[$key]}\"\ndone\n", "I was compiling a custom kernel, and I wanted to test the size of the image file.\nThese are the results:ls -la | grep vmlinux\n-rwxr-xr-x   1 root   root   8167158 May 21 12:14 vmlinux\n\ndu -h vmlinux\n3.8M    vmlinux\n\nsize vmlinux\n   text    data     bss     dec     hex filename\n2221248  676148  544768 3442164  3485f4 vmlinux\nSince all of them show different sizes, which one is closest to the actual image size?\nWhy are they different?", "I was compiling a custom kernel, and I wanted to test the size of the image file.\nThese are the results:ls -la | grep vmlinux\n-rwxr-xr-x   1 root   root   8167158 May 21 12:14 vmlinux\n\ndu -h vmlinux\n3.8M    vmlinux\n\nsize vmlinux\n   text    data     bss     dec     hex filename\n2221248  676148  544768 3442164  3485f4 vmlinux\nSince all of them show different sizes, which one is closest to the actual image size?\nWhy are they different?", "I have a folder named \"A\", and there are 3 sub-folders in it which are named \"A1\", \"A2\", \"A3\" respectively.Is there a command to show the amount of files in each sub-folders?E.g.A1     5\nA2     7\nA3    18  \nThanks.", "I have a folder containing a lot of KVM qcow2 files, they are all sparse files.\nNow I need to get the total size of folder, the qcow2 file size should be counted as apparent size(not real size).for example:image: c9f38caf104b4d338cc1bbdd640dca89.qcow2\nfile format: qcow2\nvirtual size: 100G (107374182400 bytes)\ndisk size: 3.3M\ncluster_size: 65536the image should be treated as 100G but not 3.3Moriginally I use statvfs() but it can only return real size of the folder. then I switch to 'du --apparent-size', but it's too slow given I have 10000+ files and it takes almost 5 minutes to caculate.anybody knows a fast way that can get the size of folder counting qcow2's virtual size? thank you", "I have a folder containing a lot of KVM qcow2 files, they are all sparse files.\nNow I need to get the total size of folder, the qcow2 file size should be counted as apparent size(not real size).for example:image: c9f38caf104b4d338cc1bbdd640dca89.qcow2\nfile format: qcow2\nvirtual size: 100G (107374182400 bytes)\ndisk size: 3.3M\ncluster_size: 65536the image should be treated as 100G but not 3.3Moriginally I use statvfs() but it can only return real size of the folder. then I switch to 'du --apparent-size', but it's too slow given I have 10000+ files and it takes almost 5 minutes to caculate.anybody knows a fast way that can get the size of folder counting qcow2's virtual size? thank you", "I have du redirect into a file named stdout.txt, and the file's contents read as follows(for example):4.0K    ./Makefile.am\n20K     ./dfasearch.c\n8.0K    ./dosbuf.c\n4.0K    ./egrep.sh\n84K     ./grep.c\n4.0K    ./grep.h\n8.0K    ./kwsearch.c\n36K     ./kwset.c\n4.0K    ./kwset.h\n12K     ./pcresearch.c\n4.0K    ./search.h\n4.0K    ./searchutils.c\n4.0K    ./system.h\nFrom that file, I would like to be able to display only lines above or below a given size value.  \"sort -h\" gets me part of the way there, I think, but I'm not sure how to go about culling the lines I wouldn't need.  For instance, let's say I only wanted to print lines that represented a 12K or less file, my output should look something like:4.0K    ./Makefile.am\n8.0K    ./dosbuf.c\n4.0K    ./egrep.sh\n4.0K    ./grep.h\n8.0K    ./kwsearch.c\n4.0K    ./kwset.h\n12K     ./pcresearch.c\n4.0K    ./search.h\n4.0K    ./searchutils.c\n4.0K    ./system.h\nIs there a common tool that naturally sorts by human-readable sizes and displays only the lines below (in this case) a given size?  Ideally, I'd like to have some bash code that could be used to generate this output above or below a user-provided number that could be denoted by K, MB, or GB, etc.", "I am downloading a large file with wget, which I ran in the background with wget -bqc. I wanted to see how much of the file was downloaded so I randu -sh *in the directory. (I'd also be interested to know a better way to check wget progress in this case if anyone knows...) I saw that 25 GB had been downloaded, but for several attempts afterwards it showed the same result of 25 GB. I became worried that du had somehow interfered with the download until some time later when du showed a result of 33 GB and subsequently 40 GB.In searching stackoverflow and online, I didn't find whether it is safe to use du on files being written to but I did see that it is only an estimate that can be somewhat off. However, 7-8 GB seems like a lot, particularly because it is a single file, and not a directory tree, which it seems is what causes errors in the estimate. I'd be interested to know how it makes this estimate for a single file that is being written and why I would see this result."], "chosen": ["\nYou're supplying mutually exclusive requirements, so nothing can match.\nUse an or.\nfind . -maxdepth 1 -type d \\( -size +5M -o -size -3M \\)\n\nIf that gets no hits, maybe none of the directories at -maxdepth 1 fit the conditions. Try removing that.\nfind . -type d \\( -size +5M -o -size -3M \\)\n\nI think you're not going to find a lot of directories >5M, though. find isn't reporting the same data as du. It's a lot like the ls vs du questioned asked here.\nmaybe what you want is something more like one of these.\ndu -b | awk '$1<3000000||$1>5000000' # or\ndu -b | awk '$1<3145728||$1>5242880' | sort -n\n\n", "\nWhat I would do to find dirs in range 3 to 5MiB:\nfind . -maxdepth 1 -type d -exec bash -c '\n        for d; do\n            s=$(du -s -B M \"$d\" | sed -E \"s/^([0-9]+).*/\\1/\")\n            ((s>=3 && s<=5)) && echo \"$d ${s}MiB\"\n        done\n    ' bash {} +\n\n find's -size is meant for files, not for directory.\n\nOr as far as you only treat only current directory:\ndu -sb */ | awk '$1>3000000 && $1<5000000'\n\n", "\nSince sed does greedy matching by default:\ndu -s -BM /home/user/test/test_file_check | sed 's|/.*/||'\n\nsed is going to match the longest line that matches /.*/ (ie, greedy match) and then replace it with nothing (||). NOTE: since the data includes the / character we need a different script delimiter hence the use of | as the sed script delimiter\nSimulating your du output:\n$ echo '0M      /home/user/test/test_file_check' | sed 's|/.*/||'\n0M      test_file_check\n\n", "\nSince you didn't include shopt -s nullglob, it's likely that Desktop/* didn't expand to any file which is odd unless there really are no files there, you have enabled nullglob in interactive mode, and du -sh doesn't actually display the sizes of the files in Desktop.\nIt's also likely that you're calling the script from where Desktop/ doesn't exist.\nYou can add a debug statement which prints $PWD.  You can also try running the script with bash -x.\nIn your script I suggest enabling nullglob and then modifying it so du -sh isn't called if target directory contains no files.\nSomething like:\nset -- \"$i\"/*; [[ $# -gt 0 ]] && du -sh -- \"$@\" ...\n\nAlso $@ should be quoted when being expanded.\nfor i in \"$@\"; do\n\nThis can be simplified to for i; do, but we will modify the positional parameters inside the loop so we expand \"$@\" instead.\nYou can also choose to store the expanded files inside an array as well.\n", "\nUsing file foo:\n$ cat foo\n213     sample-data     foo          # this file\n214     client-abc      bar          # some file I had in the dir\n215     some            nonexistent  # didn't have this one\n\nand the awk:\n$ gawk '                             # using GNU awk\n@load \"filefuncs\"                    # for this default extension\n!stat($3,statdata) {                 # \"returns zero upon success\"\n    a[$2]+=statdata[\"size\"]          # get the size and update array\n}\nEND {                                # in the end\n    for(i in a)                      # iterate all\n        print i,a[i]                 # and output\n}' foo foo                           # running twice for testing array grouping\n\nOutput:\nclient-abc 70\nsample-data 18\n\n", "\nTwo definition need to be understood\n1 runtime vs storetime (this is why size differs)\n2 file depth vs directory (this is why du differs)\nLook at the below example:\n[root@localhost test]# ls -l\ntotal 36\n-rw-r--r-- 1 root root   712 May 12 19:50 a.c\n-rw-r--r-- 1 root root  3561 May 12 19:42 a.h\n-rwxr-xr-x 1 root root 71624 May 12 19:50 a.out\n-rw-r--r-- 1 root root  1403 May  8 00:15 b.c\n-rw-r--r-- 1 root root  1403 May  8 00:15 c.c\n[root@localhost test]# du -abch --max-depth=1\n1.4K    ./b.c\n1.4K    ./c.c\n3.5K    ./a.h\n712     ./a.c\n70K     ./a.out\n81K     .\n81K     total\n[root@localhost test]# ls -l\ntotal 36\n-rw-r--r-- 1 root root   712 May 12 19:50 a.c\n-rw-r--r-- 1 root root  3561 May 12 19:42 a.h\n-rwxr-xr-x 1 root root 71624 May 12 19:50 a.out\n-rw-r--r-- 1 root root  1403 May  8 00:15 b.c\n-rw-r--r-- 1 root root  1403 May  8 00:15 c.c\n[root@localhost test]# size a.out\n   text    data     bss     dec     hex filename\n   3655     640      16    4311    10d7 a.out\n\nIf using size not on executable, OS will report an error.\n", "\nThey are all correct, they just show different sizes.\n\nls shows size of the file (when you open and read it, that's how many bytes you will get)\ndu shows actual disk usage which can be smaller than the file size due to holes\nsize shows the size of the runtime image of an object/executable which is not directly related to the size of the file (bss uses no bytes in the file no matter how large, the file may contain debugging information that is not part of the runtime image, etc.)\n\nIf you want to know how much RAM/ROM an executable will take excluding dynamic memory allocation, size gives you the information you need.\n", "\nTry this\nls -R /path/to/A-diretory \n\ndu -ah /path/to/A-direcory          //here it will tell the size of files too \n\nif you want to see your listing in tree like structure, use tree command, its a very powerful command having a lot of options.\nbefore that\napt-get install tree (for ubuntu)\n\ntree /path/to/A-directory\n\nYou can always refer man command to explore more of a command\n", "\nThere is no way to find out this information without stat()ing every file in the directory. It is slow if you have this many files in a single directory. stat() needs to retrieve the inode of every single file.\nAdding more memory might help due to caching.\n", "\nYou could use something like this:\nfind images/ -name \"*.qcow2\" -exec qemu-img info {} \\; | grep virtual | cut -d\"(\" -f2 | awk '{ SUM += $1} END { print SUM }'\n", "\nYou can use sort with option -h (at least with sort from the GNU coreutils version 8.25).  With this option it sorts human-readable numbers like you have them here with suffixes k, M, etc. or without a suffix.\nAfter sorting it is just a question of finding the place where to cut.\n", "\nThe operating system has to go guarantee safe access.\ndu does not estimate anything. the kernel knows the size of the file and when du asks for it that's what it learns.\nIf the file is in the range of gigabytes and the reported size is only with that granularity, it should not be a surprise that consecutive invocations show the same size - do you expect wget to fetch enough data to flip to another gigabyte in between your checks? You can try running du without sh in order to get a more accurate read.\nAlso wget will hold some amount of data in ram, but that should be negligible.\n"], "rejected": ["\nfind -type d -size does not work as you apparently think. It considers the size of the directories that ls -l shows, not the total size of the content of the directories (the files they contain). If you are interested in the total size of the content you must use du in your find command:\nfind . -maxdepth 1 -type d -execdir bash -c \\\n  's=($(du -sb \"$1\")); (( 3145728 <= s[0] && s[0] <= 5242880 ))' _ {} \\; -print\n\nNote that -size +5M -size -3M means \"strictly more than 5M and strictly less than 3M\" which, even if the find directory sizes were what you think, would obviously return nothing. Note also that, due to the rounding, -size -3M means less or equal than 2M and -size +5M means more or equal 6M.\n", "\nfind -type d -size does not work as you apparently think. It considers the size of the directories that ls -l shows, not the total size of the content of the directories (the files they contain). If you are interested in the total size of the content you must use du in your find command:\nfind . -maxdepth 1 -type d -execdir bash -c \\\n  's=($(du -sb \"$1\")); (( 3145728 <= s[0] && s[0] <= 5242880 ))' _ {} \\; -print\n\nNote that -size +5M -size -3M means \"strictly more than 5M and strictly less than 3M\" which, even if the find directory sizes were what you think, would obviously return nothing. Note also that, due to the rounding, -size -3M means less or equal than 2M and -size +5M means more or equal 6M.\n", "\nA slightly tongue in cheek approach. This navigates to the directory and does the du in the local directory, then navigates back to the previous location.\ncd /home/user/test/ && du -s -BM test_file_check && cd - > /dev/null || return\n0M      test_file_check\n\n", "\nPlease do not post so much in one question. Please one problem per question. One script per question, etc.\nMake sure to check your scripts with shellcheck. It will catch your mistakes. See https://mywiki.wooledge.org/Quotes .\n\n\nWhen run without arguments:\n\n\nfilename=(\"'\" name \"'\") inside awk script is a invalid way to pass anything with ' quotes to system() call, so you are getting unterminated ' error, as expected, because there will be 3 ' characters. Fix the AWS script, or better rewrite it in Bash, no need for awk. Maybe rewrite it all in Python or Perl.\nMoreover, tmp=(name \" \" $i); deletes tabs and multiple spaces from filenames. It's all meant to work with only nice filenames.\nThe script will break on newlines in filenames anyway.\n\n\nWhen run with arguments:\n\n\n$@ undergoes word splitting and filename expansion (topics you should research). Word splitting splits the input into words on spaces. Use \"$@\". Quote the expansions.\n\n\nWhat I want:\n\n\nYou'll be doing that with \"$@\"\n\n\nWhat I tried:\n\n\nThe variable content is irrelevant. You have to change the way you use the variable, not it's content. I.e. use quotes around the use of the variable. Not the content.\n\n\nExtra:\n\n\nYou did not quote the expansion. Use \"$i\" not $i. It's \"$i\"/*. $1 undergoes word splitting.\n\nAnd finally, after that all, your script may look like, with GNU tools:\nif (($# == 0)); then\n   set -- *\nfi\ndu -hs0 \"$@\" |\nsort -zh |\nsed -z 's/\\t/\\x00/' |\nwhile IFS= read -r -d '' size && IFS= read -r -d '' file; do\n   printf \"%s \" \"$size\";\n   ls -d \"$file\"\ndone\n\nAlso see How can I find and safely handle file names containing newlines, spaces or both? https://mywiki.wooledge.org/BashFAQ/001 .\nAlso, you can chain any statements:\nif stuff; then\n   stuff1\nelse\n   stuff2\nfi | \nsort -h |\nawk -f yourscriptrt \n\nAnd also don't repeat yourself - use bash arrays:\nargs=()\nif stuff; then\n  args=(*)\nelse\n  args=(\"$@\")\nfi\ndu -hs \"${args[@]}\" | stuff...\n\nAnd so that sort has less work to do, I would put it right after du, not after parsing.\n", "\nAssuming:\n\nyou have GNU coreutils du\nthe filenames do not contain whitespace\n\nThis has no shell loops, calls du once, and iterates over the pm_report file twice.\nfile=/tmp/pm_report.txt\n\nawk '{printf \"%s\\0\", $3}' \"$file\" \\\n| du -s --files0-from=- 2>/dev/null \\\n| awk '\n    NR == FNR {du[$2] = $1; next}\n    {client_du[$2] += du[$3]}\n    END {\n      OFS = \"\\t\"\n      for (client in client_du) print client, client_du[client]\n    }\n  ' - \"$file\"\n\n", "\nEmpirically differences happen most often for sparse files and for compressed files and can go in both directions.\n\ndu < ls\n\nSparse files contain metadata about space needed for an application, which ls reads and applies for its result, while du doesn't. For example:\ntruncate -s 1m test.dat\n\ncreates a sparse file consisting entirely of nulls without disk usage, ie. du shows 0 and ls shows 1M.\n\ndu > ls\n\nOn the other hand du can indicate, as in your case, files which might occupy a lot of space on disk (ie. they spread among lots of blocks), but not all blocks are filled, i.e. their bytesize (measured by ls) is smaller than du (looking at occupied blocks). I observed this rather prominently e.g. for some python pickle files.\n", "\nEmpirically differences happen most often for sparse files and for compressed files and can go in both directions.\n\ndu < ls\n\nSparse files contain metadata about space needed for an application, which ls reads and applies for its result, while du doesn't. For example:\ntruncate -s 1m test.dat\n\ncreates a sparse file consisting entirely of nulls without disk usage, ie. du shows 0 and ls shows 1M.\n\ndu > ls\n\nOn the other hand du can indicate, as in your case, files which might occupy a lot of space on disk (ie. they spread among lots of blocks), but not all blocks are filled, i.e. their bytesize (measured by ls) is smaller than du (looking at occupied blocks). I observed this rather prominently e.g. for some python pickle files.\n", "\nWell, you can loop over each subdirectory in ./A and output the number of files/folders contained in each subdirectory with:\nfor i in A/*; do [ -d \"$i\" ] && echo \"${i##*/}  $(ls -1 \"$i\" | wc -l)\"; done\n\nWhich just loops over all files and folders in A, and if the current name is a directory, it echos the directory name and a count of the number of files/folders in that directory.\n(note: that is a ls -\"one\" and a wc -\"ell\")\nTo include hidden files in the subdirectories use ls -a1 which is -\"a\"one\" and then subtract 2 from each total (for . and ..)\nGive that a try and let me know if you have further questions.\n", "\nModern Unix*ish OSes provide a way to retrieve the stats of all entries of a directory in one step.  This also needs to look at all inodes but probably it can be done optimized in the file system driver itself and thus might be faster.\nApparently you are not looking for a way to do this using system calls from C, so I guess a feasible approach could be to use Python.  There you have access to this feature using the function scandir() in module os.\n", "\nModern Unix*ish OSes provide a way to retrieve the stats of all entries of a directory in one step.  This also needs to look at all inodes but probably it can be done optimized in the file system driver itself and thus might be faster.\nApparently you are not looking for a way to do this using system calls from C, so I guess a feasible approach could be to use Python.  There you have access to this feature using the function scandir() in module os.\n", "\nFirst thing I'd advise you is to drop the -h, just use du -k. Once this is done, you might refer to awk just to see the entries, being larger than your desired value, as explained in this post, hereby an example:\ndu -k . | sort -n | awk '{if($1>1000000) print $1 $2}'\n\n", "\ndu doesn't estimate, it sums up.  But it has access to some file-system-internal information which might make its output be a surprise.  The various aspects should be looked up separately as they are a bit too much to explain here in detail.\n\nSparse files may make a file look bigger than it is on disk.\nHard links may make a directory tree look bigger than it is on disk.\nBlock sizes may make a file look smaller than it is on disk.\n\ndu will always print out the size a directory tree (or several) actually and really occupy on disk.  Due to various facts (the three most common are given above) this can be different from the size of the information stored in theses trees.\n"]}