{"prompt": [[], ["I have a sample file:- name: my-app\n  <<: *foo\n  installed: true\n  labels:\n    code: x.x\n    mode: y.y\n  set:\n    - name: a.b.c\n      value: abc-ab1cd2\n    - name: d.e.f\n      value: ab1cd2\nI needed sed/awk command to first match the word (my-app) in a file test.yaml and then replace the value (1234 in value: ) for -name: def and also subsequently for the value (abc-1234 in value: ) for -name: a.b.cI have tried a sed commandsed -i -e '/d.e.f/{ n; s/value: .*/value: 'value-to-change'/g }' test.yamland it just replaces the value for -name: d.e.f for all of the apps in my file. Whereas, I actually wanted to just perform the change for one app (my-app)"], [], [], [], [], ["I have file which contain followingltm pool /Common/foo_mim_pool {\n    members {\n        /Common/mim-foo-010101-1:5222 {\n            address 10.22.1.161\n        }\n    }\n}\nltm pool /Common/foo_ts_pool {\n    members {\n        /Common/ts-vx-010101-1:6698 {\n            address 10.20.1.68\n        }\n        /Common/ts-vx-010101-1:6699 {\n            address 10.20.1.68\n        }\n        /Common/ts-vx-010101-2:6698 {\n            address 10.21.1.199\n        }\n        /Common/ts-vx-010101-2:6699 {\n            address 10.21.1.199\n        }\n    }\n    monitor /Common/ts_monitor\n}\nI want to merge them in single line like following example but look like something i missing to understandltm pool /Common/foo_mim_pool { members { /Common/mim-foo-010101-1:5222 { address 10.22.1.161 } } monitor /Common/tcp}\n\nltm pool /Common/foo_ts_pool { members { /Common/ts-vx-010101-1:6698 { address 10.20.1.68 } /Common/ts-vx-010101-1:6699 { address 10.20.1.68 } /Common/ts-vx-010101-2:6698 { address 10.21.1.199 } /Common/ts-vx-010101-2:6699 { address 10.21.1.199 } } monitor /Common/ts_monitor }\nMy first attempt to use following command but its not producing what I wantpaste -d \" \" - - - - - - - - < file.txtawk 'NR%2{printf \"%s \",$0;next;}1' file.txt", "I have file which contain followingltm pool /Common/foo_mim_pool {\n    members {\n        /Common/mim-foo-010101-1:5222 {\n            address 10.22.1.161\n        }\n    }\n}\nltm pool /Common/foo_ts_pool {\n    members {\n        /Common/ts-vx-010101-1:6698 {\n            address 10.20.1.68\n        }\n        /Common/ts-vx-010101-1:6699 {\n            address 10.20.1.68\n        }\n        /Common/ts-vx-010101-2:6698 {\n            address 10.21.1.199\n        }\n        /Common/ts-vx-010101-2:6699 {\n            address 10.21.1.199\n        }\n    }\n    monitor /Common/ts_monitor\n}\nI want to merge them in single line like following example but look like something i missing to understandltm pool /Common/foo_mim_pool { members { /Common/mim-foo-010101-1:5222 { address 10.22.1.161 } } monitor /Common/tcp}\n\nltm pool /Common/foo_ts_pool { members { /Common/ts-vx-010101-1:6698 { address 10.20.1.68 } /Common/ts-vx-010101-1:6699 { address 10.20.1.68 } /Common/ts-vx-010101-2:6698 { address 10.21.1.199 } /Common/ts-vx-010101-2:6699 { address 10.21.1.199 } } monitor /Common/ts_monitor }\nMy first attempt to use following command but its not producing what I wantpaste -d \" \" - - - - - - - - < file.txtawk 'NR%2{printf \"%s \",$0;next;}1' file.txt"], ["What is the fastest way to extract a substring of interest from input such as the following?MsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nDesired output (i.e., the :-terminated string following the string MsgTrace(65/26) in this example):noopI tried the following, but without success:egrep -i \"[a-zA-Z]+\\(.*\\)[a-z]+:\"\n", "What is the fastest way to extract a substring of interest from input such as the following?MsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nDesired output (i.e., the :-terminated string following the string MsgTrace(65/26) in this example):noopI tried the following, but without success:egrep -i \"[a-zA-Z]+\\(.*\\)[a-z]+:\"\n"], ["I have some data files from a legacy system that I would like to process using Awk.  Each file consists of a list of records.  There are several different record types and each record type has a different set of fixed-width fields (there is no field separator character).  The first two characters of the record indicate the type, from this you then know which fields should follow.  A file might look something like this:AAField1Field2LongerField3\nBBField4Field5Field6VeryVeryLongField7Field8\nCCField99\nUsing Gawk I can set the FIELDWIDTHS, but that applies to the whole file (unless I am missing some way of setting this on a record-by-record basis), or I can set FS to \"\" and process the file one character at a time, but that's a bit cumbersome.Is there a good way to extract the fields from such a file using Awk?Edit: Yes, I could use Perl (or something else).  I'm still keen to know whether there is a sensible way of doing it with Awk though.", "I have some data files from a legacy system that I would like to process using Awk.  Each file consists of a list of records.  There are several different record types and each record type has a different set of fixed-width fields (there is no field separator character).  The first two characters of the record indicate the type, from this you then know which fields should follow.  A file might look something like this:AAField1Field2LongerField3\nBBField4Field5Field6VeryVeryLongField7Field8\nCCField99\nUsing Gawk I can set the FIELDWIDTHS, but that applies to the whole file (unless I am missing some way of setting this on a record-by-record basis), or I can set FS to \"\" and process the file one character at a time, but that's a bit cumbersome.Is there a good way to extract the fields from such a file using Awk?Edit: Yes, I could use Perl (or something else).  I'm still keen to know whether there is a sensible way of doing it with Awk though.", "I have some data files from a legacy system that I would like to process using Awk.  Each file consists of a list of records.  There are several different record types and each record type has a different set of fixed-width fields (there is no field separator character).  The first two characters of the record indicate the type, from this you then know which fields should follow.  A file might look something like this:AAField1Field2LongerField3\nBBField4Field5Field6VeryVeryLongField7Field8\nCCField99\nUsing Gawk I can set the FIELDWIDTHS, but that applies to the whole file (unless I am missing some way of setting this on a record-by-record basis), or I can set FS to \"\" and process the file one character at a time, but that's a bit cumbersome.Is there a good way to extract the fields from such a file using Awk?Edit: Yes, I could use Perl (or something else).  I'm still keen to know whether there is a sensible way of doing it with Awk though.", "I have some data files from a legacy system that I would like to process using Awk.  Each file consists of a list of records.  There are several different record types and each record type has a different set of fixed-width fields (there is no field separator character).  The first two characters of the record indicate the type, from this you then know which fields should follow.  A file might look something like this:AAField1Field2LongerField3\nBBField4Field5Field6VeryVeryLongField7Field8\nCCField99\nUsing Gawk I can set the FIELDWIDTHS, but that applies to the whole file (unless I am missing some way of setting this on a record-by-record basis), or I can set FS to \"\" and process the file one character at a time, but that's a bit cumbersome.Is there a good way to extract the fields from such a file using Awk?Edit: Yes, I could use Perl (or something else).  I'm still keen to know whether there is a sensible way of doing it with Awk though.", "I have some data files from a legacy system that I would like to process using Awk.  Each file consists of a list of records.  There are several different record types and each record type has a different set of fixed-width fields (there is no field separator character).  The first two characters of the record indicate the type, from this you then know which fields should follow.  A file might look something like this:AAField1Field2LongerField3\nBBField4Field5Field6VeryVeryLongField7Field8\nCCField99\nUsing Gawk I can set the FIELDWIDTHS, but that applies to the whole file (unless I am missing some way of setting this on a record-by-record basis), or I can set FS to \"\" and process the file one character at a time, but that's a bit cumbersome.Is there a good way to extract the fields from such a file using Awk?Edit: Yes, I could use Perl (or something else).  I'm still keen to know whether there is a sensible way of doing it with Awk though.", "I have some data files from a legacy system that I would like to process using Awk.  Each file consists of a list of records.  There are several different record types and each record type has a different set of fixed-width fields (there is no field separator character).  The first two characters of the record indicate the type, from this you then know which fields should follow.  A file might look something like this:AAField1Field2LongerField3\nBBField4Field5Field6VeryVeryLongField7Field8\nCCField99\nUsing Gawk I can set the FIELDWIDTHS, but that applies to the whole file (unless I am missing some way of setting this on a record-by-record basis), or I can set FS to \"\" and process the file one character at a time, but that's a bit cumbersome.Is there a good way to extract the fields from such a file using Awk?Edit: Yes, I could use Perl (or something else).  I'm still keen to know whether there is a sensible way of doing it with Awk though."], ["I need to append a UUID ( newly generated unique for each line) to each line of a file. I would prefer to use SED or AWK for this activity and take advantage of UUIDGEN executable on my linux box. I cannot figure out how to generate the the UUID for each line and append it.I have tried:awk '{print system(uuidgen) $1} myfile.csv\nsed -i -- 's/^/$(uuidgen)/g' myfile.csv\nAnd many other variations that didn't work. Can this be done with SED or AWK, or should I be investigating another solution that is not shell script based?Sincerely,\nStephen.", "I need to append a UUID ( newly generated unique for each line) to each line of a file. I would prefer to use SED or AWK for this activity and take advantage of UUIDGEN executable on my linux box. I cannot figure out how to generate the the UUID for each line and append it.I have tried:awk '{print system(uuidgen) $1} myfile.csv\nsed -i -- 's/^/$(uuidgen)/g' myfile.csv\nAnd many other variations that didn't work. Can this be done with SED or AWK, or should I be investigating another solution that is not shell script based?Sincerely,\nStephen.", "I need to append a UUID ( newly generated unique for each line) to each line of a file. I would prefer to use SED or AWK for this activity and take advantage of UUIDGEN executable on my linux box. I cannot figure out how to generate the the UUID for each line and append it.I have tried:awk '{print system(uuidgen) $1} myfile.csv\nsed -i -- 's/^/$(uuidgen)/g' myfile.csv\nAnd many other variations that didn't work. Can this be done with SED or AWK, or should I be investigating another solution that is not shell script based?Sincerely,\nStephen.", "I need to append a UUID ( newly generated unique for each line) to each line of a file. I would prefer to use SED or AWK for this activity and take advantage of UUIDGEN executable on my linux box. I cannot figure out how to generate the the UUID for each line and append it.I have tried:awk '{print system(uuidgen) $1} myfile.csv\nsed -i -- 's/^/$(uuidgen)/g' myfile.csv\nAnd many other variations that didn't work. Can this be done with SED or AWK, or should I be investigating another solution that is not shell script based?Sincerely,\nStephen.", "I need to append a UUID ( newly generated unique for each line) to each line of a file. I would prefer to use SED or AWK for this activity and take advantage of UUIDGEN executable on my linux box. I cannot figure out how to generate the the UUID for each line and append it.I have tried:awk '{print system(uuidgen) $1} myfile.csv\nsed -i -- 's/^/$(uuidgen)/g' myfile.csv\nAnd many other variations that didn't work. Can this be done with SED or AWK, or should I be investigating another solution that is not shell script based?Sincerely,\nStephen."], [], [], ["I would like to reverse the complete text from the file.\nSay if the file contains:  com.e.h/float\nI want to get output as:  float/h.e.com \nI have tried the command:  rev file.txt\nbut I have got all the reverse output: taolf/h.e.moc\nIs there a way I can get the desired output. Do let me know. Thank you.\nHere is teh link of teh sample file: Sample Text", "I would like to reverse the complete text from the file.\nSay if the file contains:  com.e.h/float\nI want to get output as:  float/h.e.com \nI have tried the command:  rev file.txt\nbut I have got all the reverse output: taolf/h.e.moc\nIs there a way I can get the desired output. Do let me know. Thank you.\nHere is teh link of teh sample file: Sample Text", "I would like to reverse the complete text from the file.\nSay if the file contains:  com.e.h/float\nI want to get output as:  float/h.e.com \nI have tried the command:  rev file.txt\nbut I have got all the reverse output: taolf/h.e.moc\nIs there a way I can get the desired output. Do let me know. Thank you.\nHere is teh link of teh sample file: Sample Text", "I would like to reverse the complete text from the file.\nSay if the file contains:  com.e.h/float\nI want to get output as:  float/h.e.com \nI have tried the command:  rev file.txt\nbut I have got all the reverse output: taolf/h.e.moc\nIs there a way I can get the desired output. Do let me know. Thank you.\nHere is teh link of teh sample file: Sample Text", "I would like to reverse the complete text from the file.\nSay if the file contains:  com.e.h/float\nI want to get output as:  float/h.e.com \nI have tried the command:  rev file.txt\nbut I have got all the reverse output: taolf/h.e.moc\nIs there a way I can get the desired output. Do let me know. Thank you.\nHere is teh link of teh sample file: Sample Text", "I would like to reverse the complete text from the file.\nSay if the file contains:  com.e.h/float\nI want to get output as:  float/h.e.com \nI have tried the command:  rev file.txt\nbut I have got all the reverse output: taolf/h.e.moc\nIs there a way I can get the desired output. Do let me know. Thank you.\nHere is teh link of teh sample file: Sample Text"], [], ["I want to printuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\nfrom my datamessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\nHow can I do this with AWK(or whatever)? Assume that my data is stored in the \"$info\" variable (single line data).Edit : single line data i mean all data represent like this messss...<input name=\"userId\" value=\"1234\" type=\"hidden\">messsss...<input ....>messssssss\nSo i can't use grep to extract interest section. ", "I want to printuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\nfrom my datamessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\nHow can I do this with AWK(or whatever)? Assume that my data is stored in the \"$info\" variable (single line data).Edit : single line data i mean all data represent like this messss...<input name=\"userId\" value=\"1234\" type=\"hidden\">messsss...<input ....>messssssss\nSo i can't use grep to extract interest section. ", "I want to printuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\nfrom my datamessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\nHow can I do this with AWK(or whatever)? Assume that my data is stored in the \"$info\" variable (single line data).Edit : single line data i mean all data represent like this messss...<input name=\"userId\" value=\"1234\" type=\"hidden\">messsss...<input ....>messssssss\nSo i can't use grep to extract interest section. ", "I want to printuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\nfrom my datamessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\nHow can I do this with AWK(or whatever)? Assume that my data is stored in the \"$info\" variable (single line data).Edit : single line data i mean all data represent like this messss...<input name=\"userId\" value=\"1234\" type=\"hidden\">messsss...<input ....>messssssss\nSo i can't use grep to extract interest section. ", "I want to printuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\nfrom my datamessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\nHow can I do this with AWK(or whatever)? Assume that my data is stored in the \"$info\" variable (single line data).Edit : single line data i mean all data represent like this messss...<input name=\"userId\" value=\"1234\" type=\"hidden\">messsss...<input ....>messssssss\nSo i can't use grep to extract interest section. ", "I want to printuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\nfrom my datamessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\nHow can I do this with AWK(or whatever)? Assume that my data is stored in the \"$info\" variable (single line data).Edit : single line data i mean all data represent like this messss...<input name=\"userId\" value=\"1234\" type=\"hidden\">messsss...<input ....>messssssss\nSo i can't use grep to extract interest section. ", "I want to printuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\nfrom my datamessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\nHow can I do this with AWK(or whatever)? Assume that my data is stored in the \"$info\" variable (single line data).Edit : single line data i mean all data represent like this messss...<input name=\"userId\" value=\"1234\" type=\"hidden\">messsss...<input ....>messssssss\nSo i can't use grep to extract interest section. "], [], ["I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f", "I have a 4 gig CSV I am trying to search to get a sub set of the CSV. I have a file csv file that contains the keywords I'm searching for (these keywords will be in the first column on the big csv).I tried this line but it ended up taking over an hour to finish. I needed to use tr to get rid of the windows return char.LC_ALL=C grep -F -i -f <(tr -d '\\r' < keywords.csv) big_csv.csv > output.csv\nAre there anyway I could optimize this? Anything I'm missing? Would it be better to use awk or another tool for this? I even thought about sorting then splitting the big csv by the first row so when when i search I could then just search the keyword by the file name and then append that to a new file. Is there a best practice for this? I'm trying to make this as POSIX as possibleAs requested here is some of the sample data.ADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nThere will be multiple entries of the first row.There's more data in the rows but it's too long to post here.the keyword file will be like thisADLV\nADVG\nAt most the keywords.csv will have 1,000 keywords. They will all be 4 letters per keyword.Here's a gist with sample data https://gist.github.com/fishnibble/9d95658c352a1acab3cec3e965defb3f"], [], [], [], ["Trying to execute remotely a bunch of commands in a perl scriptThis looks like that :    $CMD1 = \"/usr/sbin/mminfo -av -q \\\"savetime>'-1 day 18:00:00',savetime<'17:59:59'\\\" -r \\\"ssid,totalsize,nfiles,pool\\\"|grep \\\"xxxxx\\\"|/usr/bin/awk '!seen[\\$1]++'\";\n    print Dumper $CMD1;\n    $CMD = \"/usr/bin/ssh xxxx\\@$SRV \\'$CMD1\\' 2>&1\";\n    print Dumper $CMD;\n\nBut I still have problem with the $1 in the awk command, It seems to be cancelled when running.What I can see :$VAR1 = '/usr/sbin/mminfo -av -q \"savetime>\\'-1 day 18:00:00\\',savetime<\\'17:59:59\\'\" -r \"ssid,totalsize,nfiles,pool\"|grep \"xxxxxx\"|/usr/bin/awk \\'!seen[$1]++\\'';\n$VAR1 = '/usr/bin/ssh xxxxx@\\'xxxxxx\\' \\'/usr/sbin/mminfo -av -q \"savetime>\\'-1 day 18:00:00\\',savetime<\\'17:59:59\\'\" -r \"ssid,totalsize,nfiles,pool\"|grep \"xxxxx\"|/usr/bin/awk \\'!seen[$1]++\\'\\' 2>&1';\nSo the '$1' of the awk command is passed correctly to the remote but when running :    @RESU = `$CMD`;\n    print Dumper @RESU;\nI can see that my $1 is missing (or interpretated by the remote shell as a null value) :$VAR1 = 'awk: ligne de commande:1: !seen[]++\n';\n$VAR2 = 'awk: ligne de commande:1:       ^ syntax error\n';\n$VAR3 = 'awk: ligne de commande:1: error: expression indice non valide\n';\nI've tried many things like quoting or double-quoting the string, creating the string with perl 'qq' function, putting value of $CMD1 directly in $CMD and escaping quotes but no way.And of course, my awk is piped to another awk (not provided here).I don't want a solution which runs awk localy since I've millions lines returned from the 'mminfo' command.Any clue (or a better way to do that !) ?", "Trying to execute remotely a bunch of commands in a perl scriptThis looks like that :    $CMD1 = \"/usr/sbin/mminfo -av -q \\\"savetime>'-1 day 18:00:00',savetime<'17:59:59'\\\" -r \\\"ssid,totalsize,nfiles,pool\\\"|grep \\\"xxxxx\\\"|/usr/bin/awk '!seen[\\$1]++'\";\n    print Dumper $CMD1;\n    $CMD = \"/usr/bin/ssh xxxx\\@$SRV \\'$CMD1\\' 2>&1\";\n    print Dumper $CMD;\n\nBut I still have problem with the $1 in the awk command, It seems to be cancelled when running.What I can see :$VAR1 = '/usr/sbin/mminfo -av -q \"savetime>\\'-1 day 18:00:00\\',savetime<\\'17:59:59\\'\" -r \"ssid,totalsize,nfiles,pool\"|grep \"xxxxxx\"|/usr/bin/awk \\'!seen[$1]++\\'';\n$VAR1 = '/usr/bin/ssh xxxxx@\\'xxxxxx\\' \\'/usr/sbin/mminfo -av -q \"savetime>\\'-1 day 18:00:00\\',savetime<\\'17:59:59\\'\" -r \"ssid,totalsize,nfiles,pool\"|grep \"xxxxx\"|/usr/bin/awk \\'!seen[$1]++\\'\\' 2>&1';\nSo the '$1' of the awk command is passed correctly to the remote but when running :    @RESU = `$CMD`;\n    print Dumper @RESU;\nI can see that my $1 is missing (or interpretated by the remote shell as a null value) :$VAR1 = 'awk: ligne de commande:1: !seen[]++\n';\n$VAR2 = 'awk: ligne de commande:1:       ^ syntax error\n';\n$VAR3 = 'awk: ligne de commande:1: error: expression indice non valide\n';\nI've tried many things like quoting or double-quoting the string, creating the string with perl 'qq' function, putting value of $CMD1 directly in $CMD and escaping quotes but no way.And of course, my awk is piped to another awk (not provided here).I don't want a solution which runs awk localy since I've millions lines returned from the 'mminfo' command.Any clue (or a better way to do that !) ?"], ["I have three column file like this below. I want to divide column 3 by column 2 (ignoring headers) and print it in column 4. Also, I want to calculate the log2 value of column4 and print it in column5 as shown below.head my_file.txt\nthis    is header   \nchrX:73829232:-::chrX:73831065:-    76.5382 76.34220209\nchrX:73827985:-::chrX:73829067:-    60.0702 62.1887549\nchr11:18266979:+::chr11:18269194:+  15.4004 1558.282058\nI am trying by awk, is giving less output and repeated lines.awk -v OFS='\\t' 'FNR > 1 {$4 = $3 / $2}1' my_file.txt |awk -F\"\\t\" 'FNR > 1{a = log($4)/log(2); print $0\"\\t\" a} OFS=\"\\t\"'\nawk: cmd. line:1: (FILENAME=my_file.txt FNR=15) fatal: division by zero attempted\nthis is header\nchrX:73829232:-::chrX:73831065:-    76.5382 76.3422020852288    0.997439    -0.00369948\nchrX:73829232:-::chrX:73831065:-    76.5382 76.3422020852288    0.997439\nchrX:73827985:-::chrX:73829067:-    60.0702 62.1887548960591    1.03527 0.0500071\nchrX:73827985:-::chrX:73829067:-    60.0702 62.1887548960591    1.03527\nThis is my desired output.this is my desired header\nchrX:73829232:-::chrX:73831065:-    76.5382 76.34220209 0.9974392145    -0.003699170995\nchrX:73827985:-::chrX:73829067:-    60.0702 62.1887549  1.035267985 0.05000426549\nchr11:18266979:+::chr11:18269194:+  15.4004 1558.282058 101.1845185 6.66084476\n"], [], [], [], ["Can anyone help with a similar issue? I intend to print specific rows of a column using awk in for loop. I used similar codes as suggested here but could not get any output. I have read this post but I just did not get any output. I cannot figure it out.    # my input file: sfs_file.new.txt\n    57 6546 492 3844 1685 4234 2959 6586 5810 4187\n    63 6658 525 3955 1637 4356 3039 6562 5878 4086\n    65 6523 495 3978 1663 4310 2960 6515 5708 4235\n    71 6522 507 3915 1597 4282 2948 6719 5746 4172\n    58 6593 518 3965 1690 4213 2940 6527 5697 4144\n    # expected result:\n    65\n    71\n    58\n    # My codes as below\n    for i in {3..5};do\n        awk -F \" \" -v var=$i 'FNR==var {ptint $1}' sfs_file.new.txt\n    done\n", "Can anyone help with a similar issue? I intend to print specific rows of a column using awk in for loop. I used similar codes as suggested here but could not get any output. I have read this post but I just did not get any output. I cannot figure it out.    # my input file: sfs_file.new.txt\n    57 6546 492 3844 1685 4234 2959 6586 5810 4187\n    63 6658 525 3955 1637 4356 3039 6562 5878 4086\n    65 6523 495 3978 1663 4310 2960 6515 5708 4235\n    71 6522 507 3915 1597 4282 2948 6719 5746 4172\n    58 6593 518 3965 1690 4213 2940 6527 5697 4144\n    # expected result:\n    65\n    71\n    58\n    # My codes as below\n    for i in {3..5};do\n        awk -F \" \" -v var=$i 'FNR==var {ptint $1}' sfs_file.new.txt\n    done\n"], [], [], [], ["I have about 2000 files in a directory on a Linux server. In each file, the positions x-y have invoice numbers. Which is the best way to check if there are duplicates across these files and print the file names and values? A simplified version of the problem -$ cat a.txt \nxyz1234\nxyz1234\npqr4567\n$ cat b.txt \nlon9876\nlon9876\nlon4567\nIn the above 2 files, assuming that the Invoice numbers are in the position 4-8, we have duplicates - \"4567\" in a.txt and b.txt. If we have duplicates in the same file - as we have 1234 in a.txt, it is fine. No need to print that.I tried to cut the inv numbers, but the output doesn't have file names. My plan was to cut, get the file names also along with the Invoice numbers, do a unique on the output etc.", "I have about 2000 files in a directory on a Linux server. In each file, the positions x-y have invoice numbers. Which is the best way to check if there are duplicates across these files and print the file names and values? A simplified version of the problem -$ cat a.txt \nxyz1234\nxyz1234\npqr4567\n$ cat b.txt \nlon9876\nlon9876\nlon4567\nIn the above 2 files, assuming that the Invoice numbers are in the position 4-8, we have duplicates - \"4567\" in a.txt and b.txt. If we have duplicates in the same file - as we have 1234 in a.txt, it is fine. No need to print that.I tried to cut the inv numbers, but the output doesn't have file names. My plan was to cut, get the file names also along with the Invoice numbers, do a unique on the output etc."]], "chosen": [[], [["\nYou should use a YAML parser!\n\nThe reason why you shouldn't use a text parser like sed is that the YAML format has a logical structure that is represented by special symbols and formatting (like - for sequences, or indentation for hierarchy), which is unknown to text parsers (unless you write a full YAML parser in them). YAML allows various stylings like quotes or compacting subtrees to a single line, etc. Text parsers only see that actual representation, while YAML parsers robustly operate on logical relations, regardless of the formatting.\n\nHere's an example filter that works in a number of command-line YAML processors (see below). select(.name == \"my-app\").set[] traverses to every item under a field named set, which in turn is a sibling of a field named name with the value of my-app. Each of these items is updated by select(.name == \"d.e.f\").value = 1234 and select(.name == \"a.b.c\").value = \"abc-1234\" which set the value of a field named value that by itself is a sibling of a field named name with a specified value. All of this is wrapped into a map, as the top-level structure appears to be a sequence (containing several \"apps\"), so each of them can be tested for applicability.\nmap(select(.name == \"my-app\").set[] |= (\n  select(.name == \"d.e.f\").value = 1234 |\n  select(.name == \"a.b.c\").value = \"abc-1234\"\n))\n\nUse it with mikefarah/yq as yq '\u2026' test.yaml, or with kislyuk/yq as yq -y '\u2026' test.yaml, or with itchyny/gojq as gojq --yaml-input --yaml-output '\u2026' test.yaml.\n", 1]], [], [], [], [], [["\nA couple of other techniques:\n\nprint each line without a newline, counting braces.\nawk '\n    {sub(/^\\s+/, \" \"); printf \"%s\", $0} # trims leading whitespace\n    $NF == \"{\" {level++} \n    $1 == \"}\" {level--} \n    level == 0 && NR > 1 {printf \"\\n\"}\n'\n\nltm pool /Common/foo_mim_pool { members { /Common/mim-foo-010101-1:5222 { address 10.22.1.161 } }}\nltm pool /Common/foo_ts_pool { members { /Common/ts-vx-010101-1:6698 { address 10.20.1.68 } /Common/ts-vx-010101-1:6699 { address 10.20.1.68 } /Common/ts-vx-010101-2:6698 { address 10.21.1.199 } /Common/ts-vx-010101-2:6699 { address 10.21.1.199 } } monitor /Common/ts_monitor}\n\n\nSlurp the whole file as a single string, normalize all whitespace (including newlines) to a single space, and add newlines where needed. This requires the word ltm to be constant for every record and only at the start of the record.\nawk -v RS=\"\" '{gsub(/\\s+/, \" \"); gsub(/\\} ltm /, \"}\\nltm \"); print}'\n\n\n\n", 2], ["\nThis might work for you (GNU sed):\nsed '/^ltm/{:a;N;/\\n}$/!ba;y/\\n/ /;s/\\s\\+/ /g}' file\n\nFocus on lines beginning ltm and then gather up all subsequent lines until one starting and containing } only.\nThen replace all newlines by a space and all multiple white space by a single space.\n", 2]], [["\nLittle quick and dirty test on a 2469120 lines text of such a sample entry give grep -PO as winner\ntime sed -n -e 's/^MsgTrace[^)]\\{4,\\})//;t M' -e 'b' -e ':M' -e 's/:.*//p' YourFile >/dev/null\nreal    0m7.61s\nuser    0m7:10s\nsys     0m0.13s\n\ntime awk -F ':' '/^MsgTrace/{ sub( /.*)/, \"\", $1); print $1}' YourFile >/dev/null\nreal    0m17.43s\nuser    0m16.19s\nsys     0m0.17s\n\ntime grep -Po  \"[a-zA-Z]\\(.*\\)\\K[a-z]+(?=:)\" YourFile >/dev/null\nreal    0m6.72s\nuser    0m6.23s\nsys     0m0.11s\n\ntime sed -n 's/[[:alpha:]]*([^)]*)\\([[:lower:]]*\\):.*/\\1/p' YourFile >/dev/null\nreal    0m17.43s\nuser    0m16.29s\nsys     0m0.12s\n\ntime grep -Po '(?<=MsgTrace\\(65/26\\)).*?(?=:)' YourFile >/dev/null\nreal    0m16.38s\nuser    0m15.22s\nsys     0m0.15s\n\nfor @EdMorton question (i redo the same original sed to have compare value in same context of machine load). The exact string is lot faster, i imagine that sed try several combination before selecting which is the longest one for all criteria where a .*l give lot more possibility than pool is full\ntime sed -n -e 's/^MsgTrace([^)]\\{3,\\})//;T' -e 's/:.*//p' YourFile >/dev/null\nreal    0m7.28s\nuser    0m6.60s\nsys     0m0.13s\n\ntime sed -n -e 's/^[[:alpha:]]*([^)]\\{3,\\})//;T' -e 's/:.*//p' YourFile >/dev/null\nreal    0m10.44s\nuser    0m9.67s\nsys     0m0.14s\n\ntime sed -n -e 's/^[[:alpha:]]*([^)]*)//;T' -e 's/:.*//p' YourFile >/dev/null\n\nreal    0m10.54s\nuser    0m9.75s\nsys     0m0.11s\n\n", 2], ["\nYou could try this:\n$ sed -n 's/[[:alpha:]]*([^)]*)\\([[:lower:]]*\\):.*/\\1/p' file\nnoop\n\nIt's portable to all POSIX seds and doesn't employ PCREs, just BREs, so the regexp matching part at least should be fast.\nAlternatively, using GNU awk for the 3rd arg to match():\n$ awk 'match($0,/[[:alpha:]]*\\([^)]*)([[:lower:]]*):/,a){print a[1]}' file\nnoop\n\n", 2]], [["\nYou maybe can use two passes:\n1step.awk\n/^AA/{printf \"2 6 6 12\"    }\n/^BB/{printf \"2 6 6 6 18 6\"}\n/^CC/{printf \"2 8\"         }\n{printf \"\\n%s\\n\", $0}\n\n2step.awk\nNR%2 == 1 {FIELDWIDTHS=$0}\nNR%2 == 0 {print $2}\n\nAnd then \nawk -f 1step.awk sample  | awk -f 2step.awk\n\n", 8], ["\nYou probably need to suppress (or at least ignore) awk's built-in field separation code, and use a program along the lines of:\nawk '/^AA/ { manually process record AA out of $0 }\n     /^BB/ { manually process record BB out of $0 }\n     /^CC/ { manually process record CC out of $0 }' file ...\n\nThe manual processing will be a bit fiddly - I suppose you'll need to use the substr function to extract each field by position, so what I've got as one line per record type will be more like one line per field in each record type, plus the follow-on printing.\nI do think you might be better off with Perl and its unpack feature, but awk can handle it too, albeit verbosely.\n", 5], ["\nHopefully this will lead you in the right direction. Assuming your multi-line records are guaranteed to be terminated by a 'CC' type row you can pre-process your text file using simple if-then logic.  I have presumed you require fields1,5 and 7 on one row and a sample awk script would be.\nBEGIN {\n        field1=\"\"\n        field5=\"\"\n        field7=\"\"\n}\n{\n    record_type = substr($0,1,2)\n    if (record_type == \"AA\")\n    {\n        field1=substr($0,3,6)\n    }\n    else if (record_type == \"BB\")\n    {\n        field5=substr($0,9,6)\n        field7=substr($0,21,18)\n    }\n    else if (record_type == \"CC\")\n    {\n        print field1\"|\"field5\"|\"field7\n    }\n}\n\nCreate an awk script file called program.awk and pop that code into it.  Execute the script using :\nawk -f program.awk < my_multi_line_file.txt \n\n", 4], ["\nCould you use Perl and then select an unpack template based on the first two chars of the line?\n", 4], ["\nOne awk idea using an array to keep track of the different FIELDWIDTHS formats:\nawk '\nBEGIN { fw[\"AA\"] = \"2 6 6 12\"                     # predefined FIELDWIDTHS\n        fw[\"BB\"] = \"2 6 6  6 18 6\"\n        fw[\"CC\"] = \"2 7\"\n      }\n      { FIELDWIDTHS = fw[substr($0,1,2)]          # dynamically define FIELDWIDTHS based on 1st two characters\n        $0 = $0                                   # force reparse of input line based on new FIELDWIDTHS\n        print \"#############\",$0\n        for (i=1;i<=NF;i++)\n            print \"field #\"i,\":\",$i\n      }\n' input.txt\n\nThis generates:\n############# AAField1Field2LongerField3\nfield #1 : AA\nfield #2 : Field1\nfield #3 : Field2\nfield #4 : LongerField3\n############# BBField4Field5Field6VeryVeryLongField7Field8\nfield #1 : BB\nfield #2 : Field4\nfield #3 : Field5\nfield #4 : Field6\nfield #5 : VeryVeryLongField7\nfield #6 : Field8\n############# CCField99\nfield #1 : CC\nfield #2 : Field99\n\n", 3], ["\nBetter use some fully featured scripting language like perl or ruby.\n", 1]], [["\nJust tweaking the syntax on your attempt, something like this should work:\nawk '(\"uuidgen\" | getline uuid) > 0 {print uuid, $0} {close(\"uuidgen\")}' myfile.csv\n\nFor example:\n$ cat file\na\nb\nc\n\n$ awk '(\"uuidgen\" | getline uuid) > 0 {print uuid, $0} {close(\"uuidgen\")}' file\n52a75bc9-e632-4258-bbc6-c944ff51727a a\n24c97c41-d0f4-4cc6-b0c9-81b6d89c5b77 b\n76de9987-a60f-4e3b-ba5e-ae976ab53c7b c\n\nThe right solution is to use other shell commands though since the awk isn't buying you anything:\n$ xargs -n 1 printf \"%s %s\\n\" $(uuidgen) < file\n763ed28c-453f-47f4-9b1b-b2f972b2cc7d a\n763ed28c-453f-47f4-9b1b-b2f972b2cc7d b\n763ed28c-453f-47f4-9b1b-b2f972b2cc7d c\n\n", 5], ["\nTry this\nawk '{ \"uuidgen\" |& getline u; print u, $1}' myfile.csv\n\nif you want to append instead of prepend change the order of print.\n", 4], ["\nUsing xargs is simpler here:\npaste -d \" \" myfile.csv <(xargs -I {} uuidgen {} < myfile.csv)\n\nThis will call uuidgen for each line of myfile.csv\n", 2], ["\nYou can use paste and GNU sed: \npaste <(sed 's/.*/uuidgen/e' file) file\nThis uses the GNU execute extension e to generate a UUID per line, then paste pastes the text back together. Use the -d paste flag to change the delimiter from the default tab, to whatever you want.\n", 2], ["\nUsing bash, this will create a file outfile.txt with a concatenated uuid:\nNOTE: Please run which bash to verify the location of your copy of bash on your system.  It may not be located in the same location used in the script below.\n#!/usr/local/bin/bash\n\nwhile IFS= read -r line\ndo\n    uuid=$(uuidgen)\n    echo \"$line $uuid\" >> outfile.txt\ndone < myfile.txt\n\nmyfile.txt:\njohn,doe\nmary,jane\nalbert,ellis\nbob,glob\nfig,newton\n\noutfile.txt\njohn,doe 46fb31a2-6bc5-4303-9783-85844a4a6583\nmary,jane a14bb565-eea0-47cd-a999-90f84cc8e1e5\nalbert,ellis cfab6e8b-00e7-420b-8fe9-f7655801c91c\nbob,glob 63a32fd1-3092-4a72-8c24-7b01c400820c\nfig,newton 63d38ad9-5553-46a4-9f24-2e19035cc40d\n\n", 1]], [], [], [["\nIs it possible to use Perl?\nperl -nlE 'say reverse(split(\"([/.])\",$_))'  f\n\nThis one-liner reverses all the lines of f, according to PO's criteria.\nIf prefer a less parentesis version:\nperl -nlE 'say reverse split \"([/.])\"' f \n\n", 5], ["\nWith your shown samples please try following awk code.\necho 'com.e.h/float' |\nawk '\n  BEGIN{ FS=OFS=\"/\" }\n  {\n    num=split($1,arr,\".\")\n    val=\"\"\n    for(i=num;i>0;i--){\n       val=(val?val \".\":\"\") arr[i]\n    }\n    print $2,val\n  }\n'\n\n", 3], ["\nFor portability, this can be done using any awk (not just GNU) using substrings:\n$ awk '{\n    while (match($0,/[[:alnum:]]+/)) {\n      s=substr($0,RLENGTH+1,1) substr($0,1,RLENGTH) s;\n      $0=substr($0,RLENGTH+2)\n    } print s\n  }' <<<\"com.e.h/float\"\n\nThis steps through the string grabbing alphanumeric strings plus the following character, reversing the order of those two captured pieces, and prepending them to an output string.\n", 2], ["\nYou can use sed and tac:\nstr=$(echo 'com.e.h/float' | sed -E 's/(\\W+)/\\n\\1\\n/g' | tac | tr -d '\\n')\n\necho \"$str\"\nfloat/h.e.com\n\n\nUsing sed we insert \\n before and after all non-word characters.\nUsing tac we reverse the output lines.\nUsing tr we strip all new lines.\n\n\nIf you have gnu-awk then you can do all this in a single awk command using 4 argument split function call that populates split strings and delimiters separately:\nawk '{\n   s = \"\"\n   split($0, arr, /\\W+/, seps)\n   for (i=length(arr); i>=1; i--)\n      s = s seps[i] arr[i]\n   print s\n}' file\n\nFor non-gnu awk, you can use:\nawk '{\n   r = $0\n   i = 0\n   while (match(r, /[^a-zA-Z0-9_]+/)) {\n      a[++i] = substr(r, RSTART, RLENGTH) substr(r, 0, RSTART-1)\n      r = substr(r, RSTART+RLENGTH)\n   }\n   s = r\n   for (j=i; j>=1; j--)\n      s = s a[j]\n   print s\n}' file\n\n", 1], ["\nUsing GNU awk's split, splitting from separators . and /, define more if you wish.\n$ cat program.awk\n{\n    for(n=split($0,a,\"[./]\",s); n>=1; n--)               # split to a and s, use n from split\n        printf \"%s%s\", a[n], (n==1?ORS:s[(n-1)])         # printf it pretty\n}\n\nRun it:\n$ echo com.e.h/float | awk -f program.awk \nfloat/h.e.com\n\nEDIT:\nIf you want to run it as one-liner:\nawk '{for(n=split($0,a,\"[./]\",s); n>=1; n--); printf \"%s%s\", a[n], (n==1?ORS:s[(n-1)])}' foo.txt\n\n", 0], ["\nTry this:\necho $(echo ruof eerht owt eno | fmt -1 | tac | rev)\n\nreturns\none two three four\nAlternatively:\necho $(echo ruof eerht owt eno | rev | fmt -1 | tac)\n\nreturns\nfour three two one\n", 0]], [], [["\nThis part should probably be a comment on Pax's answer, but it got a bit long for that little box. I'm thinking 'single line data' means you don't have any newlines in your variable at all? Then this will work:\necho \"$info\" | sed -n -r '/<input/s/<input +name=\"([^\"]+)\" +value=\"([^\"]+)\"[^>]*>[^<]*/\\1 = \\2\\n/gp'\n\nNotes on interesting bits:\n - -n means don't print by default - we'll say when to print with that p at the end.\n\n-r means extended regex\n/<input/ at the beginning makes sure we don't even bother to work on lines that don't contain the desired pattern\nThat \\n at the end is there to ensure all records end up on separate lines - any original newlines will still be there, and the fastest way to get rid of them is to tack on a '| grep .' on the end - you could use some sed magic but you wouldn't be able to understand it thirty seconds after you typed it in.\n\nI can think of ways to do this in awk, but this is really a job for sed (or perl!).\n", 4], ["\nTo process variables that contain more than one line, you need to put the variable name in double quotes:\necho \"$info\"|sed 's/^\\(<input\\( \\)name\\(=\\)\"\\([^\"]*\\)\" value=\"\\([^\"]*\\)\"\\)\\?.*/\\4\\2\\3\\2\\5/'\n\n", 3], ["\nusing perl\ncat file | perl -ne 'print($1 . \"=\" . $2 . \"\\n\") if(/name=\"(.*?)\".*value=\"(.*?)\"/);'\n\n", 2], ["\nIMO, parsing HTML should be done with a proper HTML/XML parser.  For example, Ruby has an excellent package, Nokogiri, for parsing HTML/XML:\nruby -e '\n    require \"rubygems\"\n    require \"nokogiri\"\n    doc = Nokogiri::HTML.parse(ARGF.read)\n    doc.search(\"//input\").each do |node|\n        atts = node.attributes\n        puts \"%s = %s\" % [atts[\"name\"], atts[\"value\"]]\n    end\n' mess.html\n\nproduces the output you're after\n", 2], ["\nI'm not sure I understand your \"single line data\" comment but if this is in a file, you can just do something like:\ncat file\n    | grep '^<input '\n    | sed 's/^<input name=\"//'\n    | sed 's/\" value=\"/ = /'\n    | sed 's/\".*$//'\n\nHere's the cut'n'paste version:\ncat file | grep '^<input ' | sed 's/^<input name=\"//' | sed 's/\" value=\"/ = /' | sed 's/\".*$//'\n\nThis turns:\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n<input name=\"userId\" value=\"1234\" type=\"hidden\"> messsssssssssssssssssss\n<input name=\"userid\" value=\"12345\" type=\"hidden\"> messssssssssssssssssss\n<input name=\"timestamp\" value=\"88888888\" type=\"hidden\"> messssssssssssss\n<input name=\"js\" value=\"abc\" type=\"hidden\"> messssssssssssssssssssssssss\nmessssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n\nquite happily into:\nuserId = 1234\nuserid = 12345\ntimestamp = 88888888\njs = abc\n\nThe grep simply extracts the lines you want while the sed commandsrespectively:\n\nstrip off up to the first quote.\nreplace the section between the name and value with an \"=\".\nremove everything following the value closing quote (including that quote).\n\n", 1], ["\nAWK:\nBEGIN {\n  # Use record separator \"<\", instead of \"\\n\".\n  RS = \"<\"\n  first = 1\n}\n\n# Skip the first record, as that begins before the first tag\nfirst {\n  first = 0\n  next\n}\n\n/^input[^>]*>/ { #/\n  # make sure we don't match outside of the tag\n  end = match($0,/>/)\n\n  # locate the name attribute\n  pos = match($0,/name=\"[^\"]*\"/)\n  if (pos == 0 || pos > end) { next }\n  name = substr($0,RSTART+6,RLENGTH-7)\n\n  # locate the value attribute\n  pos = match($0,/value=\"[^\"]*\"/)\n  if (pos == 0 || pos > end) { next }\n  value = substr($0,RSTART+7,RLENGTH-8)\n\n  # print out the result\n  print name \" = \" value\n}\n\n", 1], ["\nTools like awk and sed can be used together with XMLStarlet and HTML Tidy to parse HTML.\n", 0]], [], [["\nThis is how you'd do it in awk. I'd be interested to see if the duration is longer or shorter.\nawk -F, '\n  NR==FNR { words[$1]; next }\n  $1 in words\n' keywords.csv big_csv.csv > output.csv\n\nIt works by reading the first file into an array using the common \"first file\" test, then checking that array as a key to the second file.\nI can update this further if you add more detail to your question.\n", 4], ["\nAssumptions:\n\nfirst field (both files) does not contain commas and is not wrapped in double quotes\ndata from keywords.csv may have windows/dos line endings (\\r)\n\nSetup:\n$ cat keywords.csv                         # run through unix2dos to add \"\\r\"\nADLV\nADVG\n\n$ cat big_csv.csv\nADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\nWXYZ,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\n\nA basic awk approach:\nawk -F, '                                  # input field delimiter is a comma; for 1st file this implies entire line == 1st field\nFNR==NR { sub(/\\r$/,\"\"); a[$0]; next }     # 1st file: strip off \"\\r\", save line as index in array a[]; skip to next line of input (from 1st file)\n$1 in a                                    # 2nd file: if 1st field is an index in array a[] then print current line to stdout\n' keywords.csv big_csv.csv > output.csv\n\nThis generates:\n$ cat output.csv\nADLV,-1.741774,0.961072,-0.751392,-0.935572,-2.269994,1.081103,-0.831244,1.540083,0.474326,-1.322924,2.199037,-0.919939,0.641496,-0.584152,0.729028,0.608351,-0.522026,0.966026,-0.793949,-1.623368,1.16177,-0.642438,-0.675811,-0.214964,-2.263053,2.188642,0.302449,0.770106\n\n", 2], ["\nAssumption: key is in column 1 in both files (big.csv, keys.txt from your sample data)\nawk 'NR==FNR { a[$1] = 1; next } a[$1] {print $1,$2}' keys.txt FS=\\, big.csv\n\ngives:\nSDGA -1.678247\nSDSV -2.140182\nWDGV -1.31453\n\n(for the sake of readability I just printed the first two columns for matching keys)\nI'm not a native speaker (English), so for an explanation of NR, FNR, Arrays and all the stuff going on here, I would like to refer to this discussion.\nHope this helps!\n", 2], ["\nThis can also be done in Ruby:\nruby -e '\n# split(/\\R/) works the same with DOS or Unix line endings\nkeys=File.open(ARGV[0]).read.split(/\\R/).map(&:downcase).to_set\nFile.open(ARGV[1]).each_line{|line| \n    tst=line.split(/,/,2)[0].downcase\n    puts line if keys.include?(tst)\n}\n' sample_keyword.csv sample_input.csv >out.csv\n\nThere is no particular advantage to doing the basic example in Ruby. In fact I would do it in awk if the actual use is only what you have described. Ruby is potentially faster but often times not.\nHowever, you can do things like output to a different format (JSON, XML, complex csv) that are challenging in awk more easily done in Ruby.\nYou can replicate curl internally in Ruby and read your gist examples directly:\nruby -e '\nrequire \"net/http\"\nrequire \"uri\"\n\nuri1 = URI.parse(\"https://gist.githubusercontent.com/fishnibble/9d95658c352a1acab3cec3e965defb3f/raw/21fc5153a0b78cdb3eab88c72d700cdf74f20ae7/sample_keyword.csv\")\nkeys = Net::HTTP.get(uri1).split(/\\R/).map(&:downcase).to_set\n\n# This can be done in a streaming mode for huge data...\nuri2 = URI.parse(\"https://gist.githubusercontent.com/fishnibble/9d95658c352a1acab3cec3e965defb3f/raw/21fc5153a0b78cdb3eab88c72d700cdf74f20ae7/sample_input.csv\")\nNet::HTTP.get(uri2).split(/\\R/).each{|line|\n    tst=line.split(/,/,2)[0].downcase\n    puts line if keys.include?(tst)\n}' >out.csv\n\nThat is where you want Ruby / Python / Perl since that stream is hard to do in awk.  You can also mount external servers or read ftp, etc where that is challenging with awk alone.\n", 2], ["\n\nAre there anyway I could optimize this? Anything I'm missing?\n\nYou commanded GNU grep to look for keyword in whole line. This is unnecessary as you want to find lines having said keyword in 1st column, for CSV without quotes in 1st column this would mean line starting with keyword followed by comma (,) character.\n\nWould it be better to use awk or another tool for this?\n\nIf you have limited time you should prepare smaller example and then measure various solutions. As awk solution were already shown I will propose GNU sed solution by reworking your keywords file, say keywords.txt\nADLV\nADVG\n\ninto sed file by doing sed -e 's/^/\\/^/' -e 's/[\\r\\n]*$/,\\/p/' keywords.txt > file.sed\n/^ADLV,/p\n/^ADVG,/p\n\nwhich will create file.sed which could be used as follows\nsed -n -f file.sed file.csv\n\nExplanation: -n disengage default print action -f file.sed execute commands in file.sed, in this case these are just prints.\n(tested in GNU sed 4.8)\n", 1]], [], [], [], [["\nEven if you can not use modules in your final environment, you may be able to use them in your local machine. In that case you can use them to quote the command programmatically and then just copy and paste the quoted string into the script you are developing. For instance:\nuse strict;\nuse warnings;\nuse Net::OpenSSH;\n\nmy $quoted_cmd1 = Net::OpenSSH->shell_quote('/usr/sbin/mminfo', '-av',\n                       -q => q(savetime>'-1 day 18:00:00',savetime<'17:59:59'),\n                       -r => 'ssid,totalsize,nfiles,pool',\n                       \\\\'|',\n                       'grep', 'xxxxx',\n                       \\\\'|',\n                       '/usr/bin/awk', '!seen[$1]++');\n\nmy $SRV = \"foo\";\nmy $quoted_cmd = Net::OpenSSH->shell_quote('/usr/bin/ssh', \"xxxx\\@$SRV\",    \n                                           $quoted_cmd1,\n                                           \\\\'2>&1');\nprint \"$quoted_cmd\\n\";\n\nWhich outputs...\n/usr/bin/ssh xxxx@foo '/usr/sbin/mminfo -av -q '\\''savetime>'\\''\\'\"''\"'-1 day 18:00:00'\\''\\'\"''\"',savetime<'\\''\\'\\''17:59:59\\'\\'' -r ssid,totalsize,nfiles,pool | grep xxxxx | /usr/bin/awk '\\''!seen[$1]++'\\' 2>&1\n\n\n", 2], ["\nIf you are executing scripts in Perl via ssh using the awk pipeline, then your awk variable must be escaped \\\\\\$1 (three backslashes $1)\n", 1]], [["\nPlease use:\nawk 'NR==1 {print $0, \"column4\", \"column5\"; next} {print $0, $3/$2, log($3/$2)/log(2)}' my_file.txt\n\nFor output_file:\nawk 'NR==1 {print $0, \"column4\", \"column5\"; next} {print $0, $3/$2, log($3/$2)/log(2)}' my_file.txt > output_file.txt\n\nOutput:\nthis    is header    column4 column5\nchrX:73829232:-::chrX:73831065:-    76.5382 76.34220209 0.997439 -0.00369917\nchrX:73827985:-::chrX:73829067:-    60.0702 62.1887549 1.03527 0.0500043\nchr11:18266979:+::chr11:18269194:+  15.4004 1558.282058 101.185 6.66084\n\nEDIT:\nCheck first dividing by non-zero:\nawk 'NR==1 {print $0, \"column4\", \"column5\"; next} $2!=0 {print $0, $3/$2, log($3/$2)/log(2)} $2==0 {print $0, \"undefined\", \"undefined\"}' my_file.txt > output_file.txt\n\n", 2]], [], [], [], [["\nother alternatives...\n$ sed -n '3,5s/ .*//p' file\n\nor\n$ awk 'NR==3,NR==5{print $1}' file\n\n", 2], ["\nAs @ufopilot pointed out in a comment you have ptint instead of print. Awk takes that as an unpopulated variable name, concatenates it with $1, and then does nothing with the result, hence no output.\nNo need for a shell loop, though:\n$ awk 'NR==3{f=1} f{print $1} NR==5{exit}' file\n65\n71\n58\n\nWe exit when NR is 5 for efficiency, no point reading the rest of the file and doing nothing with it.\n", 0]], [], [], [], [["\nAs an alternate, here is a Ruby to do that:\nruby -lne 'BEGIN{files=Hash.new {|h,k| h[k] = Set.new()} }\nfiles[$_[3..6]]<<$<.file.path\nEND{\n    files.each{|inv,names| puts \"#{inv}=>#{names.join\",\"}\" if names.length>1} \n}\n' a.txt ba.txt\n\nOr with GNU awk (for the ENDFILE pattern) you can do:\ngawk '\nBEGIN { FS = OFS = \",\" }\n\nFNR == 1 { split(\"\", fn, FS) } # clear array\n\n{\n    key = substr($0, 4, 4)\n    if (! (key in fn)) {\n        fn[key]\n    }\n}\n\nENDFILE {\n    for (e in fn) {\n        idx[e] = idx[e] ? idx[e] OFS FILENAME : FILENAME\n    }\n}\n\nEND {\n    for (e in idx) {\n        n = split(idx[e], a, FS)\n        if (n > 1) {\n            print e \"=>\" idx[e]\n        }\n    }\n}\n' *.txt\n\nWith the example, either prints:\n4567=>a.txt,ba.txt\n\n", 1], ["\nThis might work for you (GNU sed, sort, uniq, ...):\necho *.txt |\nxargs -n1 sed -E 's/...(.*)/\\1 &/p;F;d' |\nsed 'N;s/\\n/ /' |\nsort -u -k1n -k3 |\nuniq -w4 -D\n\nEcho file names.\nUse xargs to apply a sed script to each file that splits the invoice in two and also appends the file name.\nUse sed to join the invoices to the file names.\nSort the result, removing duplicates for the same file.\nPrint only the invoices that are duplicates in invoice/filename order.\n", 0]]], "rejected": [[], [["\nThis might work for you (GNU sed):\nsed '/^- name:/{h;b}\n     /- name: a\\.b\\.c/{n;G;s/\\(value:\\).*my-app/\\1 abc-1234/;P;d}\n     /- name: d\\.e\\.f/{n;G;s/\\(value:\\).*my-app/\\1 1234/;P;d}' file\n\nCopy the app name into the hold space.\nIf a subsequent line contains the name a.b.c, print it and fetch the next line and append the copied app name line.\nIf that line contains value: and also my-app, replace the value with the required value and then print the first line only and delete the rest.\n", 0]], [], [], [], [], [["\nThis might be what you're trying to do, using any POSIX awk:\n$ awk '{ORS=(/^}$/ ? RS : OFS); sub(/^[[:space:]]+/,\"\")} 1' file\nltm pool /Common/foo_mim_pool { members { /Common/mim-foo-010101-1:5222 { address 10.22.1.161 } } }\nltm pool /Common/foo_ts_pool { members { /Common/ts-vx-010101-1:6698 { address 10.20.1.68 } /Common/ts-vx-010101-1:6699 { address 10.20.1.68 } /Common/ts-vx-010101-2:6698 { address 10.21.1.199 } /Common/ts-vx-010101-2:6699 { address 10.21.1.199 } } monitor /Common/ts_monitor }\n\n", -3], ["\nThis might be what you're trying to do, using any POSIX awk:\n$ awk '{ORS=(/^}$/ ? RS : OFS); sub(/^[[:space:]]+/,\"\")} 1' file\nltm pool /Common/foo_mim_pool { members { /Common/mim-foo-010101-1:5222 { address 10.22.1.161 } } }\nltm pool /Common/foo_ts_pool { members { /Common/ts-vx-010101-1:6698 { address 10.20.1.68 } /Common/ts-vx-010101-1:6699 { address 10.20.1.68 } /Common/ts-vx-010101-2:6698 { address 10.21.1.199 } /Common/ts-vx-010101-2:6699 { address 10.21.1.199 } } monitor /Common/ts_monitor }\n\n", -3]], [["\ngrep by default returns the entire line when a match is found on a given input line.\nWhile option -o restricts the output to only that part of the line that the regex matched, that is still not enough in this case, because you want a substring of that match.\nHowever, since you're on Linux, you can use GNU grep's -P option (for support of PCREs, Perl-compatible regular expression), which allows extracting a submatch by way of features such as \\K (drop everything matched so far) and (?=...) (a look-ahead assertion that does not contribute to the match):\n$ grep -Po  \"[a-zA-Z]\\(.*\\)\\K[a-z]+(?=:)\" <<'EOF'\nMsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nEOF\nnoop  # output\n\n\nOptional background information:\nEd Morton points out (in a since-deleted comment) that GNU grep's man page still calls the -P option \"highly experimental\" that may \"warn of unimplemented features\", but the option has been around for years, and in practice I have yet to see a warning or a performance problem - YMMV.  \nIn the case at hand, the above command even outperforms sed and awk solutions - see NeronLeVelu's helpful performance comparison.\nThe interesting article Ed points to discusses a potential performance problem that can surface with regex engines such as used by grep -P (via the PCRE library), Perl itself, and many other widely used (and mature) regex engines, such as in Python, Ruby, and PHP: \n\nIn short: the recursive backtracking algorithm employed by these engines can result in severe performance degradation with \"pathological\" regexes that string together long sequences of subexpressions with variable-length quantifiers, such as (a longer version of) a?a?a?a?aaaa to match aaaa.\nThe article argues that backtracking is only truly required when a regex contains backreferences, and that a different, much faster algorithm should be employed in their absence. \n\n", 0], ["\ngrep by default returns the entire line when a match is found on a given input line.\nWhile option -o restricts the output to only that part of the line that the regex matched, that is still not enough in this case, because you want a substring of that match.\nHowever, since you're on Linux, you can use GNU grep's -P option (for support of PCREs, Perl-compatible regular expression), which allows extracting a submatch by way of features such as \\K (drop everything matched so far) and (?=...) (a look-ahead assertion that does not contribute to the match):\n$ grep -Po  \"[a-zA-Z]\\(.*\\)\\K[a-z]+(?=:)\" <<'EOF'\nMsgTrace(65/26)noop:user=xxx=INBOX:cmd=534\nImapFetchComplete(56/39)user=xxxxxxxxxx\nEOF\nnoop  # output\n\n\nOptional background information:\nEd Morton points out (in a since-deleted comment) that GNU grep's man page still calls the -P option \"highly experimental\" that may \"warn of unimplemented features\", but the option has been around for years, and in practice I have yet to see a warning or a performance problem - YMMV.  \nIn the case at hand, the above command even outperforms sed and awk solutions - see NeronLeVelu's helpful performance comparison.\nThe interesting article Ed points to discusses a potential performance problem that can surface with regex engines such as used by grep -P (via the PCRE library), Perl itself, and many other widely used (and mature) regex engines, such as in Python, Ruby, and PHP: \n\nIn short: the recursive backtracking algorithm employed by these engines can result in severe performance degradation with \"pathological\" regexes that string together long sequences of subexpressions with variable-length quantifiers, such as (a longer version of) a?a?a?a?aaaa to match aaaa.\nThe article argues that backtracking is only truly required when a regex contains backreferences, and that a different, much faster algorithm should be employed in their absence. \n\n", 0]], [["\nWhat about 2 scripts? E.g. 1st script inserts field separators based on the first characters, then the 2nd should process it?\nOr first of all define some function in your AWK script, which splits the lines into variables based on the input - I would go this way, for the possible re-usage.\n", 0], ["\nWhat about 2 scripts? E.g. 1st script inserts field separators based on the first characters, then the 2nd should process it?\nOr first of all define some function in your AWK script, which splits the lines into variables based on the input - I would go this way, for the possible re-usage.\n", 0], ["\nWhat about 2 scripts? E.g. 1st script inserts field separators based on the first characters, then the 2nd should process it?\nOr first of all define some function in your AWK script, which splits the lines into variables based on the input - I would go this way, for the possible re-usage.\n", 0], ["\nWhat about 2 scripts? E.g. 1st script inserts field separators based on the first characters, then the 2nd should process it?\nOr first of all define some function in your AWK script, which splits the lines into variables based on the input - I would go this way, for the possible re-usage.\n", 0], ["\nWhat about 2 scripts? E.g. 1st script inserts field separators based on the first characters, then the 2nd should process it?\nOr first of all define some function in your AWK script, which splits the lines into variables based on the input - I would go this way, for the possible re-usage.\n", 0], ["\nWhat about 2 scripts? E.g. 1st script inserts field separators based on the first characters, then the 2nd should process it?\nOr first of all define some function in your AWK script, which splits the lines into variables based on the input - I would go this way, for the possible re-usage.\n", 0]], [["\nThese all seem pretty slow. Ruby can do it right quick.\nruby -r securerandom -ne 'print SecureRandom.uuid + \" \" + $_' file\n\n", 0], ["\nThese all seem pretty slow. Ruby can do it right quick.\nruby -r securerandom -ne 'print SecureRandom.uuid + \" \" + $_' file\n\n", 0], ["\nThese all seem pretty slow. Ruby can do it right quick.\nruby -r securerandom -ne 'print SecureRandom.uuid + \" \" + $_' file\n\n", 0], ["\nThese all seem pretty slow. Ruby can do it right quick.\nruby -r securerandom -ne 'print SecureRandom.uuid + \" \" + $_' file\n\n", 0], ["\nThese all seem pretty slow. Ruby can do it right quick.\nruby -r securerandom -ne 'print SecureRandom.uuid + \" \" + $_' file\n\n", 0]], [], [], [["\nNot sure exactly what you're going for, but this should work.\nUsing any awk:\nawk 'BEGIN {OFS=FS=\"/\"}\n    {print $2,\n     substr($1,length($1)) \".\"\n     substr($1,length($1)-2,1) \".\"\n     substr($1,1,3)}' input.txt\n\n", 0], ["\nNot sure exactly what you're going for, but this should work.\nUsing any awk:\nawk 'BEGIN {OFS=FS=\"/\"}\n    {print $2,\n     substr($1,length($1)) \".\"\n     substr($1,length($1)-2,1) \".\"\n     substr($1,1,3)}' input.txt\n\n", 0], ["\nNot sure exactly what you're going for, but this should work.\nUsing any awk:\nawk 'BEGIN {OFS=FS=\"/\"}\n    {print $2,\n     substr($1,length($1)) \".\"\n     substr($1,length($1)-2,1) \".\"\n     substr($1,1,3)}' input.txt\n\n", 0], ["\nNot sure exactly what you're going for, but this should work.\nUsing any awk:\nawk 'BEGIN {OFS=FS=\"/\"}\n    {print $2,\n     substr($1,length($1)) \".\"\n     substr($1,length($1)-2,1) \".\"\n     substr($1,1,3)}' input.txt\n\n", 0], ["\nNot sure exactly what you're going for, but this should work.\nUsing any awk:\nawk 'BEGIN {OFS=FS=\"/\"}\n    {print $2,\n     substr($1,length($1)) \".\"\n     substr($1,length($1)-2,1) \".\"\n     substr($1,1,3)}' input.txt\n\n", 0], ["\nNot sure exactly what you're going for, but this should work.\nUsing any awk:\nawk 'BEGIN {OFS=FS=\"/\"}\n    {print $2,\n     substr($1,length($1)) \".\"\n     substr($1,length($1)-2,1) \".\"\n     substr($1,1,3)}' input.txt\n\n", 0]], [], [["\nHere is a short awk oneliner using bash :\nawk 'BEGIN{ FS=\"\\\"\"; RS=\"<\";}/\\=/{print $2,\" = \", $4;}' <(printf \"%s\" ${info})\n\nExplanation :\nRS=\"<\" -- break the text into records (-lines)\nFS=\"\\\"\" -- break records into fields by \"\n/\\=/ -- choose lines containing =    \n\n{print $2,\" = \", $4;} -- print 2nd and 4th field separated with spaces and =\n", 0], ["\nHere is a short awk oneliner using bash :\nawk 'BEGIN{ FS=\"\\\"\"; RS=\"<\";}/\\=/{print $2,\" = \", $4;}' <(printf \"%s\" ${info})\n\nExplanation :\nRS=\"<\" -- break the text into records (-lines)\nFS=\"\\\"\" -- break records into fields by \"\n/\\=/ -- choose lines containing =    \n\n{print $2,\" = \", $4;} -- print 2nd and 4th field separated with spaces and =\n", 0], ["\nHere is a short awk oneliner using bash :\nawk 'BEGIN{ FS=\"\\\"\"; RS=\"<\";}/\\=/{print $2,\" = \", $4;}' <(printf \"%s\" ${info})\n\nExplanation :\nRS=\"<\" -- break the text into records (-lines)\nFS=\"\\\"\" -- break records into fields by \"\n/\\=/ -- choose lines containing =    \n\n{print $2,\" = \", $4;} -- print 2nd and 4th field separated with spaces and =\n", 0], ["\nHere is a short awk oneliner using bash :\nawk 'BEGIN{ FS=\"\\\"\"; RS=\"<\";}/\\=/{print $2,\" = \", $4;}' <(printf \"%s\" ${info})\n\nExplanation :\nRS=\"<\" -- break the text into records (-lines)\nFS=\"\\\"\" -- break records into fields by \"\n/\\=/ -- choose lines containing =    \n\n{print $2,\" = \", $4;} -- print 2nd and 4th field separated with spaces and =\n", 0], ["\nHere is a short awk oneliner using bash :\nawk 'BEGIN{ FS=\"\\\"\"; RS=\"<\";}/\\=/{print $2,\" = \", $4;}' <(printf \"%s\" ${info})\n\nExplanation :\nRS=\"<\" -- break the text into records (-lines)\nFS=\"\\\"\" -- break records into fields by \"\n/\\=/ -- choose lines containing =    \n\n{print $2,\" = \", $4;} -- print 2nd and 4th field separated with spaces and =\n", 0], ["\nHere is a short awk oneliner using bash :\nawk 'BEGIN{ FS=\"\\\"\"; RS=\"<\";}/\\=/{print $2,\" = \", $4;}' <(printf \"%s\" ${info})\n\nExplanation :\nRS=\"<\" -- break the text into records (-lines)\nFS=\"\\\"\" -- break records into fields by \"\n/\\=/ -- choose lines containing =    \n\n{print $2,\" = \", $4;} -- print 2nd and 4th field separated with spaces and =\n", 0], ["\nHere is a short awk oneliner using bash :\nawk 'BEGIN{ FS=\"\\\"\"; RS=\"<\";}/\\=/{print $2,\" = \", $4;}' <(printf \"%s\" ${info})\n\nExplanation :\nRS=\"<\" -- break the text into records (-lines)\nFS=\"\\\"\" -- break records into fields by \"\n/\\=/ -- choose lines containing =    \n\n{print $2,\" = \", $4;} -- print 2nd and 4th field separated with spaces and =\n", 0]], [], [["\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", -1], ["\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", -1], ["\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", -1], ["\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", -1], ["\nUsing any awk, it sounds like all you need is:\nawk -F, 'NR==FNR{sub(/\\r$/,\"\"); keys[$1]; next} $1 in keys' keywords.csv big_csv.csv\n\nThe sub(/\\r$/,\"\") is there because you had tr -d '\\r' < keywords.csv in your code - if you don't have DOS line endings in your keywords file then you don't need that.\nI see you also have -i in your grep command, though, which implies you need to make the matching case-insensitive - if so then this is what you need, still using any awk:\nawk -F, '{key=tolower($1)} NR==FNR{sub(/\\r$/,\"\"); keys[key]; next} key in keys' keywords.csv big_csv.csv\n\nThe grep line you tried would not only take a long time to finish but it could also produce incorrect output as it's searching across the whole line of big_csv, not just the first field, and so it would generate a false match if the keyword appeared in some other position in a line, and it would also generate a false match if the keyword you wanted happened to be a substring of some other keyword.\n", -1]], [], [], [], [["\nYou might want to break it into smaller pieces for readability, and use the multi-arg invocation of system to avoid perl having to spawn a shell. The q() function goes a long way toward avoiding quoting hell.\n$mminfo = q{/usr/sbin/mminfo -av -q \"savetime>'-1 day 18:00:00',savetime<'17:59:59'\" -r \"ssid,totalsize,nfiles,pool\"};\n$awk = q{/usr/bin/awk '/xxxxx/ && !seen[$1]++');\nprint Dumper [$mminfo, $awk];\n\n@cmd = ( \"/usr/bin/ssh\", \"xxxx\\@$SRV\", \"$mminfo | $awk\" );\nprint Dumper \\@cmd;\n\nsystem @cmd;\n\n", 0], ["\nYou might want to break it into smaller pieces for readability, and use the multi-arg invocation of system to avoid perl having to spawn a shell. The q() function goes a long way toward avoiding quoting hell.\n$mminfo = q{/usr/sbin/mminfo -av -q \"savetime>'-1 day 18:00:00',savetime<'17:59:59'\" -r \"ssid,totalsize,nfiles,pool\"};\n$awk = q{/usr/bin/awk '/xxxxx/ && !seen[$1]++');\nprint Dumper [$mminfo, $awk];\n\n@cmd = ( \"/usr/bin/ssh\", \"xxxx\\@$SRV\", \"$mminfo | $awk\" );\nprint Dumper \\@cmd;\n\nsystem @cmd;\n\n", 0]], [["\nYou can try the below command:\nawk -v OFS='\\t' 'FNR==1 {print $0, \"col4\", \"col5\"; next} {if ($2 == 0) {$4 = \"NaN\"; $5 = \"NaN\"} else {$4 = $3 / $2; $5 = log($4) / log(2)}} 1' my_file.txt\n\nBasically, the above command:\n\nchecks for the first row (header) and adds \"col4\" and \"col5\" as column names.\nFor the remaining rows, it calculates the division and log2 values if column 2 is not zero; otherwise, it sets \"NaN\" as the value for columns 4 and 5. By setting the values of columns 4 and 5 to \"NaN\" when column 2 is zero, the script avoids division by zero errors and provides a clear indication that the result of the calculation is not a valid number.\n\nCODE DEMO\n", 0]], [], [], [], [["\nSorry, the real data are a bit more complex than the previously posted example data. Real data are with the last col separated by \"\\t\" as below so requiring -F \"\\t\" I think.\n## real data: sfs_file.new1.txt\n5 6 3 1 42 29 66 10 47  ft_sim_1_1_MSFS.obs\n\nMy final intention is to use this awk-for-loop as below.\nfor i in {1,4..5};do\n    ## replace pop AA,CC,DD -> filename in col2\n    sfs1=`awk -F \"\\t\" -v var=$i 'FNR==var {print $2}' sfs_file.new1.txt`\n    sed -i -e \"3s/AA/$sfs1/\" -e \"12s/CC/$sfs1/\" -e \"20s/DD/$sfs1/\" ft_sim_MSFS_1_\"$i\"_MSFS.blueprint\n    ## replace pop BB -> filename in col1\n    sfs2=`awk -F \"\\t\" -v var=$i 'FNR==var {print $1}' sfs_file.new1.txt`\n    sed -i \"7s/BB/$sfs2/\" ft_sim_MSFS_1_\"$i\"_MSFS.blueprint\ndone\n\n", 0], ["\nSorry, the real data are a bit more complex than the previously posted example data. Real data are with the last col separated by \"\\t\" as below so requiring -F \"\\t\" I think.\n## real data: sfs_file.new1.txt\n5 6 3 1 42 29 66 10 47  ft_sim_1_1_MSFS.obs\n\nMy final intention is to use this awk-for-loop as below.\nfor i in {1,4..5};do\n    ## replace pop AA,CC,DD -> filename in col2\n    sfs1=`awk -F \"\\t\" -v var=$i 'FNR==var {print $2}' sfs_file.new1.txt`\n    sed -i -e \"3s/AA/$sfs1/\" -e \"12s/CC/$sfs1/\" -e \"20s/DD/$sfs1/\" ft_sim_MSFS_1_\"$i\"_MSFS.blueprint\n    ## replace pop BB -> filename in col1\n    sfs2=`awk -F \"\\t\" -v var=$i 'FNR==var {print $1}' sfs_file.new1.txt`\n    sed -i \"7s/BB/$sfs2/\" ft_sim_MSFS_1_\"$i\"_MSFS.blueprint\ndone\n\n", 0]], [], [], [], [["\nPerl to the rescue!\nperl -lne '\n    $in_file{ substr $_, 3, 4 }{$ARGV} = 1;\n    END {\n        for $invoice (%in_file) {\n            print join \"\\t\", $invoice, keys %{ $in_file{$invoice} }\n                if keys %{ $in_file{$invoice} } > 1;\n        }\n    }\n' -- *txt\n\n\n-n reads the input files line by line, running the code for each;\n-l removes newlines from the input and adds them to printed lines;\n$ARGV contains the name of the currently open file;\nwe build a hash of hashes, the first level key is the invoice number, the second level key is the file it was found in;\nsee substr for the details on how to extract the invoice number;\nat the end of all input, we print the keys (i.e. invoice numbers) that have more than one file associated with themselves.\n\n", -2], ["\nPerl to the rescue!\nperl -lne '\n    $in_file{ substr $_, 3, 4 }{$ARGV} = 1;\n    END {\n        for $invoice (%in_file) {\n            print join \"\\t\", $invoice, keys %{ $in_file{$invoice} }\n                if keys %{ $in_file{$invoice} } > 1;\n        }\n    }\n' -- *txt\n\n\n-n reads the input files line by line, running the code for each;\n-l removes newlines from the input and adds them to printed lines;\n$ARGV contains the name of the currently open file;\nwe build a hash of hashes, the first level key is the invoice number, the second level key is the file it was found in;\nsee substr for the details on how to extract the invoice number;\nat the end of all input, we print the keys (i.e. invoice numbers) that have more than one file associated with themselves.\n\n", -2]]]}